---
title: "Spatial Interpolation"
subtitle: <h4 style="font-style:normal">GEO 200CN - Quantitative Geography</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
date: <h4 style="font-style:normal">Spring 2020</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: yeti
    code_folding: show
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```

Spatial interpolation is the prediction of exact values of attributes at unsampled locations from measurements made at control points in the same study area. In this lab guide, we will learn how to run the deterministic spatial interpolation methods described in OSU Ch. 9. The objectives of this lab are as follows

1. Learn how to interpolate values using proximity polygons
2. Learn how to interpolate values using nearest neighbor local spatial average
3. Learn how to interpolate values using inverse distance weighted

To help us accomplish these learning objectives, we will use precipitation data from weather stations in California. Our goal is to create a raster surface of California total precipitation values. Note that much of this lab guide has been adapted from rspatial.org.

<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

We'll be using one new package in this lab.  First, install the package if you haven't already.

```{r eval = FALSE}
if (!require("gstat")) install.packages("gstat")
```

Second, load all necessary packages.

```{r warning=FALSE, message=FALSE}
library(sp)
library(rgdal)
library(dismo)
library(gstat)
```

<div style="margin-bottom:25px;">
</div>
## **Bringing in the data**
\

To demonstrate spatial interpolation we'll be relying on a dataset containing monthly temperature data for weather stations in California.  Download the  file *interpolation.zip* from Canvas. It contains all the files that will be used in this guide. The data are also located on Canvas in the Lab and Assignments Week 9 folder. Bring in the dataset *precipitation.csv*


```{r}
d <- read.csv("precipitation.csv")
head(d)
```

We have monthly precipitation levels for 456 weather stations in California.  Rather than monthly levels, we'll be interpolating annual precipitation levels.  So, we add up the 12 months using `rowSums` to create the year total variable *prec*.  Plot to see what the distribution looks like.


```{r}
d$prec <- rowSums(d[, c(6:17)])
plot(sort(d$prec), ylab='Annual precipitation (mm)', las=1, xlab='Stations')
```

Next, let's map the annual levels.  Bring in a California county layer.

```{r results = "hide"}
CA <- readOGR("counties.shp")
```

And then map the stations by levels of total precipitation.  We'll need to make spatial points out of the station data.

```{r}
dsp <- SpatialPoints(d[,4:3], proj4string=CRS("+proj=longlat +datum=NAD83"))
dsp <- SpatialPointsDataFrame(dsp, d)
# define groups for mapping
cuts <- c(0,200,300,500,1000,3000)
# set up a palette of interpolated colors
blues <- colorRampPalette(c('yellow', 'orange', 'blue', 'dark blue'))
pols <- list("sp.polygons", CA, fill = "lightgray")
spplot(dsp, 'prec', cuts=cuts, col.regions=blues(5), sp.layout=pols, pch=20, cex=2)
```

Transform longitude/latitude to planar coordinates, using the commonly used coordinate reference system for California (“Teale Albers”) to assure that our interpolation results will align with other data sets we have.

```{r}
TA <- CRS("+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000 +datum=NAD83 +units=m +ellps=GRS80 +towgs84=0,0,0")
dta <- spTransform(dsp, TA)
cata <- spTransform(CA, TA)
```


<div style="margin-bottom:25px;">
</div>
## **Mean model**
\

Our goal is to interpolate (estimate for unsampled locations) the station precipitation values across California. The simplest way would be to use the mean of all observed values. This is described on page 253 in OSU. It's not a great interpolator because it does not take into account anything spatial.  We'll consider it a “Null-model” that we can compare other more refined approaches to. We’ll use the Root Mean Square Error (RMSE) as evaluation statistic. We created a nifty RMSE function a few lab guides ago. Let's bring it back here.

```{r}
RMSE <- function(observed, predicted) {
  sqrt(mean((predicted - observed)^2, na.rm=TRUE))
}
```

Get the RMSE for the Null-model

```{r}
null <- RMSE(mean(dsp$prec), dsp$prec)
null
```


<div style="margin-bottom:25px;">
</div>
## **Proximity polygons**
\

OSU's first spatial interpolator is proximity polygons, which is described on page 254.  Another term for this is "nearest neighbour” interpolation. We use the function `vornoi()` to create the polygons surrounding the control points.

```{r}
v <- voronoi(dta)
plot(v)
```


Looks weird. Let’s confine this to California

```{r}
ca <- aggregate(cata)
vca <- intersect(v, ca)
spplot(vca, 'prec', col.regions=rev(get_col_regions()))
```

Much better. These are polygons. Our ultimate goal is to create a continuous raster surface of precipitation values. We can ‘rasterize’ the results like this.  

```{r}
r <- raster(cata, res=10000)
vr <- rasterize(vca, r, 'prec')
plot(vr)
```

How good is the proximity polygon model? We evaluate the RMSE using 5-fold cross validation.

```{r results="hide"}
#insert comment
set.seed(5132015)
#insert comment
kf <- kfold(nrow(dta))
#insert comment
rmsepp <- rep(NA, 5)
#insert comment
for (k in 1:5) {
  #insert comment
  test <- dta[kf == k, ]
  #insert comment
  train <- dta[kf != k, ]
  #insert comment
  v <- voronoi(train)
  #insert comment
  p <- extract(v, test)
  #insert comment
  rmsepp[k] <- RMSE(test$prec, p$prec)
}
#insert comment
rmsepp
#insert comment
mean(rmsepp)
#insert comment
1 - (mean(rmsepp) / null)
```


<br>

<p class="comment", style="font-style:normal">**Question 1**: Add comments to the above code chunk to describe what each step is doing.  </p>

<p class="comment", style="font-style:normal">**Question 2**: How does the proximity-polygon approach compare to the NULL model? </p>

<p class="comment", style="font-style:normal">**Question 3**: You would not typically use proximity polygons for rainfall data. Why? For what kind of data would you use them? </p>


<br>

<div style="margin-bottom:25px;">
</div>
## **Local Spatial Average**
\

The Local Spatial Average method (OSU page 255) generalizes the proximity polygon method by considering more than one neighbor.  The method is sometimes called nearest neighbor interpolation.  Here we do nearest neighbor interpolation considering five neighbors.

We can use the `gstat()` function located in the **gstat** package for this. First we fit the following model. `~1` means “intercept only”. In the case of spatial data, that would be only ‘x’ and ‘y’ coordinates are used. We set the maximum number of points to 5, and the “inverse distance power” `idp` to zero, such that all five neighbors are equally weighted (so the message you will get that says you are running inverse distance weighting interpolation is not true in this case).

```{r}
gs <- gstat(formula=prec~1, locations=dta, nmax=5, set=list(idp = 0))
nn <- interpolate(r, gs)
```

Plot our results

```{r}
nnmsk <- mask(nn, vr)
plot(nnmsk)
```

Cross validate the result (k = 5). Note that we can use the `predict()` method to get predictions for the locations of the test points.


```{r}
rmsenn <- rep(NA, 5)
for (k in 1:5) {
  test <- dta[kf == k, ]
  train <- dta[kf != k, ]
  gscv <- gstat(formula=prec~1, locations=train, nmax=5, set=list(idp = 0))
  p <- predict(gscv, test)$var1.pred
  rmsenn[k] <- RMSE(test$prec, p)
}
rmsenn
mean(rmsenn)
1 - (mean(rmsenn) / null)
```

Better than the null, but better than proximity polygons?

<div style="margin-bottom:25px;">
</div>
## **Inverse Distance Weighted**
\


A more commonly used method is “inverse distance weighted” interpolation. This method is described starting on page 257 in OSU. The only difference with the nearest neighbor approach is that points that are further away get less weight in predicting a value a location.  Use the `gstat()` function again, but this time do not set `idp` to 0.

```{r}
gs <- gstat(formula=prec~1, locations=dta)
idw <- interpolate(r, gs)
```

Plot the predictions.

```{r}
idwr <- mask(idw, vr)
plot(idwr)
```

<br>

<p class="comment", style="font-style:normal">**Question 4**: IDW generated rasters tend to have a noticeable artifact. What is it? </p>

<br>

Cross validate (k = 5). We can predict to the locations of the test points

```{r}
rmseidw <- rep(NA, 5)
for (k in 1:5) {
  test <- dta[kf == k, ]
  train <- dta[kf != k, ]
  gs <- gstat(formula=prec~1, locations=train)
  p <- predict(gs, test)
  rmseidw[k] <- RMSE(test$prec, p$var1.pred)
}
rmseidw
mean(rmseidw)
1 - (mean(rmseidw) / null)
```

<br>

<p class="comment", style="font-style:normal">**Question 5**: Inspect the arguments used for and make a map of the IDW model below. What other name could you give to this method (IDW with these parameters)? Why? </p>

<br>

```{r}
gs2 <- gstat(formula=prec~1, locations=dta, nmax=1, set=list(idp=1))
```


***

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)