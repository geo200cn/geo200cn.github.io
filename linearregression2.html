<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Professor Noli Brazil" />

<meta name="date" content="2024-04-22" />

<title>More Linear Regression</title>

<script src="site_libs/header-attrs-2.22/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
      .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">GEO 200CN: Spring 2024</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
<li>
  <a href="hw_guidelines.html">Assignment Guidelines</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="eda.html">Exploratory Data Analysis</a>
    </li>
    <li>
      <a href="inference.html">Statistical Inference</a>
    </li>
    <li>
      <a href="hypothesis.html">Hypothesis Testing</a>
    </li>
    <li>
      <a href="linearregression.html">Linear Regression</a>
    </li>
    <li>
      <a href="linearregression2.html">More Linear Regression</a>
    </li>
    <li>
      <a href="logistic.html">Logistic Regression</a>
    </li>
    <li>
      <a href="introspatial.html">Intro to Spatial Data</a>
    </li>
    <li>
      <a href="pointpatterns.html">Point Pattern Analysis</a>
    </li>
    <li>
      <a href="spatialautocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="spatialreg.html">Spatial Regression</a>
    </li>
    <li>
      <a href="prediction.html">Prediction Modelling</a>
    </li>
    <li>
      <a href="modelselection.html">Model Selection</a>
    </li>
    <li>
      <a href="interpolation.html">Spatial Interpolation</a>
    </li>
    <li>
      <a href="kriging.html">Kriging</a>
    </li>
    <li>
      <a href="regtrees.html">Regression Trees</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Other
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tips.html">R Tips</a>
    </li>
    <li>
      <a href="data_wrangling.html">Data Wrangling</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">More Linear Regression</h1>
<h3 class="subtitle"><font size="4">GEO 200CN - Quantitative
Geography</font></h3>
<h4 class="author">Professor Noli Brazil</h4>
<h4 class="date">April 22, 2024</h4>

</div>


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: normal;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>
<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>
<p><br />
</p>
<p>We’re in the second leg of our journey into linear regression. In <a
href="https://geo200cn.github.io/linearregression.html">last lab</a>, we
learned about simple linear regression and model fit. In this lab, we go
through the R functions for running regression diagnostics and multiple
linear regression. The objectives of this lab are as follows</p>
<ol style="list-style-type: decimal">
<li>Learn how to run diagnostic tools to evaluate Ordinary Least Squares
(OLS) regression assumptions</li>
<li>Learn how to run and evaluate a multiple linear regression
model</li>
<li>Learn how to detect multicollinearity</li>
</ol>
<p>To help us accomplish these learning objectives, we will continue
examining the association between neighborhood characteristics and
housing eviction rates in the Sacramento metropolitan area. We’ll be
closely following the material presented in Handout 4.</p>
<div style="margin-bottom:25px;">

</div>
<div id="installing-and-loading-packages" class="section level2">
<h2><strong>Installing and loading packages</strong></h2>
<p><br />
</p>
<p>We’ll be using a couple of new packages in this lab. First, you’ll
need to install them if you have not already done so.</p>
<pre class="r"><code>install.packages(c(&quot;GGally&quot;, &quot;lmtest&quot;, &quot;car&quot;))</code></pre>
<p>Load these packages and others we will need for this lab.</p>
<pre class="r"><code>library(MASS)
library(tidyverse)
library(sf)
library(gridExtra)
library(GGally)
library(car)
library(lmtest)</code></pre>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="bringing-in-the-data" class="section level2">
<h2><strong>Bringing in the data</strong></h2>
<p><br />
</p>
<p>Download the csv file <em>sac_metro_eviction.csv</em> located on
Canvas in the Week 4 Lab and Assignments folder. Read in the csv file
using the <code>read_csv()</code> function.</p>
<pre class="r"><code>sac_metro &lt;- read_csv(&quot;sac_metro_eviction.csv&quot;)</code></pre>
<p>2017 eviction rate case data were downloaded from the <a
href="https://evictionlab.org/">Eviction Lab website</a>. Socioeconomic
and demographic data were downloaded from the 2013-2017 <a
href="https://www.census.gov/programs-surveys/acs">American Community
Survey</a>. A record layout of the data can be found <a
href="https://raw.githubusercontent.com/geo200cn/data/master/sacmetroeviction.txt">here</a>.
Our research question in this lab is: What ecological characteristics
are associated with neighborhood eviction rates in the Sacramento
metropolitan area?</p>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="checking-ols-assumptions" class="section level2">
<h2><strong>Checking OLS assumptions</strong></h2>
<p><br />
</p>
<p>The linear regression handout outlines the core assumptions that need
to be met in order to obtain unbiased regression estimates from an OLS
model. The handout also goes through several diagnostic tools for
examining whether an OLS model breaks these assumptions. In this
section, we will go through how to run most of these diagnostics in
R.</p>
<p>Let’s also create a fake dataset that meets the OLS assumptions to
act as a point of comparison along the way. We’ll call this the
<em>goodreg</em> model.</p>
<pre class="r"><code>#sets a seed so the following is reproducible
set.seed(08544)
x &lt;-rnorm(5000, mean = 7, sd = 1.56)# just some normally distributed data

## We&#39;re establishing here a linear relationship,
## What&#39;s this &quot;true&quot; linear relationship we&#39;re setting up?
## So that y = 12 - .4x + some normally distributed error values
y &lt;- 12 - 0.4*x +rnorm(5000, mean = 0, sd = 1)

goodreg &lt;- lm(y ~ x)
summary(goodreg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7188 -0.6658  0.0158  0.6944  3.5102 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 12.033221   0.064450  186.71   &lt;2e-16 ***
## x           -0.402152   0.008986  -44.75   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9932 on 4998 degrees of freedom
## Multiple R-squared:  0.2861, Adjusted R-squared:  0.2859 
## F-statistic:  2003 on 1 and 4998 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>To be clear, we know the exact functional form of this model, all OLS
assumptions should be met, and therefore this model should pass all
diagnostics.</p>
<p>In the last lab guide, we ran a simple regression model using
ordinary least squares (OLS) to estimate the relationship between
eviction rates per 100 renting households and percent unemployed at the
neighborhood level. Let’s run this model using <code>lm()</code> and
save its results in an object named <em>lm1</em>. Our dependent variable
is <em>evict</em> and the independent variable is <em>punemp</em>.</p>
<pre class="r"><code>#eliminate scientific notation
options(scipen=999)

lm1 &lt;- lm(evict ~ punemp, 
          data = sac_metro)

#results
summary(lm1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = evict ~ punemp, data = sac_metro)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.1572 -0.8287 -0.3145  0.5179  6.4098 
## 
## Coefficients:
##             Estimate Std. Error t value           Pr(&gt;|t|)    
## (Intercept)  0.95715    0.12183   7.856 0.0000000000000375 ***
## punemp       0.09143    0.01462   6.254 0.0000000010381178 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.2 on 397 degrees of freedom
## Multiple R-squared:  0.08967,    Adjusted R-squared:  0.08738 
## F-statistic: 39.11 on 1 and 397 DF,  p-value: 0.000000001038</code></pre>
<div style="margin-bottom:25px;">

</div>
<div id="normally-distributed-errors" class="section level3">
<h3><strong>Normally distributed errors</strong></h3>
<p><br />
</p>
<p>We can rely on several tools for testing the errors are normally
distributed assumption. The first is a histogram of residuals. We can
extract the residuals from an <em>lm</em> object using the function
<code>resid()</code>. We will need to use the residuals for other
diagnostics, so let’s save them into the <em>sac_metro</em> data frame
under the variable <em>resid</em> using the <code>mutate()</code>
function.</p>
<pre class="r"><code>sac_metro &lt;- sac_metro %&gt;%
            mutate(resid = resid(lm1))</code></pre>
<p>The order of the tracts in<code>resid(lm1)</code> is the same as the
order of the tracts in <em>sac_metro</em> and that’s why we were able to
directly column bind it like we did in the above code.</p>
<p>Now, we create a histogram of residuals using our best bud
<code>ggplot()</code> which we met in <a
href="https://geo200cn.github.io/eda.html#Data_visualization">Week
2</a>.</p>
<pre class="r"><code>sac_metro %&gt;%
  ggplot() + 
  geom_histogram(mapping = (aes(x=resid))) + 
  xlab(&quot;Absolute Residuals&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-7-1.png" /><!-- --></p>
<p>We’re trying to see if its shape is that of a normal distribution
(bell curve). This is a histogram of absolute residuals. To get a
histogram of standardized residuals use the function
<code>stdres()</code>, where the main argument is our model results
<em>lm1</em></p>
<pre class="r"><code>sac_metro %&gt;%
  ggplot() + 
  geom_histogram((aes(x=stdres(lm1)))) + 
  xlab(&quot;Standardized Residuals&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-8-1.png" /><!-- --></p>
<p>You can also plot a histogram of the studentized residuals using the
function <code>rstudent()</code></p>
<pre class="r"><code>sac_metro %&gt;% ggplot() + 
  geom_histogram((aes(x=rstudent(lm1)))) + 
  xlab(&quot;Studentized Residuals&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-9-1.png" /><!-- --></p>
<p>For comparison, the following is what the residuals from our
simulated good data look like</p>
<pre class="r"><code>ggplot() + geom_histogram(aes(x = stdres(goodreg))) +
  xlab(&quot;Standardized Residuals&quot;) +
  ggtitle(&quot;Distribution of Residuals - Simulated Data&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-10-1.png" /><!-- --></p>
<p>You can also examine a normal probability plot, also known as a Q-Q
plot, to check error normality. Use the function <code>qqnorm()</code>
and just plug in the model residuals. The function <code>qqline()</code>
adds the line for what normally distributed data should theoretically
follow.</p>
<pre class="r"><code>qqnorm(sac_metro$resid)
qqline(sac_metro$resid,col=&quot;red&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-11-1.png" /><!-- --></p>
<p>In short, if the points of the plot do not closely follow a straight
line, this would suggest that the data do not come from a normal
distribution. What does the Q-Q plot look like for our good model?</p>
<pre class="r"><code>qqnorm(stdres(goodreg))
qqline(stdres(goodreg),col=&quot;red&quot;)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-12-1.png" /><!-- --></p>
<p><br></p>
<p class="comment">
<strong>Question 1</strong>: What do you conclude by visually examining
the histogram and Q-Q plot of the <em>lm1</em> residuals?
</p>
<p><br></p>
<p>Histograms and Q-Q plots give a nice visual presentation of the
residual distribution, however if we are interested in formal hypothesis
testing, there are a number of options available. A commonly used test
is the Shapiro–Wilk test, which is implemented in R using the function
<code>shapiro.test()</code>. The null is that the data are normally
distributed. Our good model <em>goodreg</em> should not reject the
null</p>
<pre class="r"><code>shapiro.test(resid(goodreg))</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(goodreg)
## W = 0.9994, p-value = 0.1026</code></pre>
<p>What about our simple linear regression model?</p>
<pre class="r"><code>shapiro.test(resid(lm1))</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(lm1)
## W = 0.88656, p-value &lt; 0.00000000000000022</code></pre>
<p>What’s the conclusion?</p>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="residual-scatterplots" class="section level3">
<h3><strong>Residual scatterplots</strong></h3>
<p><br />
</p>
<p>You can use a plot of the residuals against the fitted values for
checking both the linearity and homoscedasticity assumptions. We should
look for two things in this plot.</p>
<ul>
<li>At any fitted value, the mean of the residuals should be roughly 0.
If this is the case, the linearity assumption is valid. For this reason,
we generally add a horizontal line in the plot at y = 0 to emphasize
this point.</li>
<li>At every fitted value, the spread of the residuals should be roughly
the same. If this is the case, the homoscedasticity (equal variance)
assumption is valid.</li>
</ul>
<p>We know what diagnostic plots should look like when we have good
data. But what about for bad data? Below is an example of some bad data
that breaks the linearity assumption. Don’t worry too much about the
intricacies of the code below - were just trying creating simulated data
that is deliberately not normal so you can see what nonlinearity looks
like in the context of the diagnostic tools we’ve been running.</p>
<pre class="r"><code>set.seed(42)
sim_3 = function(sample_size = 500) {
  x = runif(n = sample_size) * 5
  y = 3 + 5 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 5)
  data.frame(x, y)
}

sim_data_3 = sim_3()
badreg = lm(y ~ x, 
            data = sim_data_3)</code></pre>
<p>Here is the residual vs fitted values plot for <em>badreg</em>. What
do you see that let’s you know this model breaks OLS assumptions?</p>
<pre class="r"><code>plot(fitted(badreg), resid(badreg), col = &quot;grey&quot;, pch = 20,
     xlab = &quot;Fitted&quot;, ylab = &quot;Residuals&quot;, main = &quot;Data from Bad Model&quot;)
abline(h = 0, col = &quot;darkorange&quot;, lwd = 2)</code></pre>
<p><img
src="linearregression2_files/figure-html/unnamed-chunk-16-1.png" /><!-- --></p>
<p><br></p>
<p class="comment">
<strong>Question 2</strong>: Create a residual against fitted value plot
as described by Handout 4 for the <em>lm1</em> model. Do the same for
the <em>goodreg</em> model. What do you conclude from these plots?
</p>
<p><br></p>
<div style="margin-bottom:25px;">

</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><strong>Multiple linear regression</strong></h2>
<p><br />
</p>
<p>A simple linear regression is, well, too simple. You’ll want to add
more variables in your model to</p>
<ol style="list-style-type: decimal">
<li>Obtain more precise predictions</li>
<li>Examine the relationship between the response and more than one
variable</li>
<li>Control for variables that are confounding the relationship between
<em>X</em> and <em>Y</em>.</li>
</ol>
<p>Reason (3) is particularly important for avoiding violations of the
OLS assumptions. Let’s go through this reason first to help motivate why
to include more than one variable in the model.</p>
<div style="margin-bottom:25px;">

</div>
<div id="controlling-for-variables" class="section level3">
<h3><strong>Controlling for variables</strong></h3>
<p><br />
</p>
<p>The most common reason why your model is breaking OLS assumptions is
because you’ve failed to include a variable that is confounding the
relationship between your primary independent variable(s) and the
outcome. Here, we are interested in examining the impact of a variable X
on the outcome Y controlling for the impact of another variable Z. In
other words, we don’t really care about the effect of Z, but simply want
to control for it so we can get an unbiased estimate of the effect of X.
In this case, Z is a confounding variable. Let’s try to make clear what
we mean by confounding. Here are three ways to define a confounding
variable, all saying the same thing, but in different ways.</p>
<ul>
<li><p>Confounding variables or confounders are often defined as
variables that correlate (positively or negatively) with both the
dependent variable and the independent variable</p></li>
<li><p>A confounder is an extraneous variable whose presence affects the
variables being studied so that the results do not reflect the actual
relationship between the variables under study.</p></li>
<li><p>A third variable, not the dependent (outcome) or main
independent(exposure) variable of interest, that distorts the observed
relationship between the exposure and outcome.</p></li>
</ul>
<p>Confounding can have serious consequences for your results. Going
back to our case study of Sacramento let’s say we ran a simple linear
regression of eviction rates on the median age of housing units in the
neighborhood.</p>
<pre class="r"><code>summary(lm(evict ~  medage, 
           data = sac_metro))</code></pre>
<pre><code>## 
## Call:
## lm(formula = evict ~ medage, data = sac_metro)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8200 -0.8991 -0.2816  0.5505  6.3894 
## 
## Coefficients:
##             Estimate Std. Error t value      Pr(&gt;|t|)    
## (Intercept)  1.16320    0.18663   6.233 0.00000000117 ***
## medage       0.01104    0.00425   2.597       0.00975 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.247 on 397 degrees of freedom
## Multiple R-squared:  0.01671,    Adjusted R-squared:  0.01423 
## F-statistic: 6.745 on 1 and 397 DF,  p-value: 0.009753</code></pre>
<p>We would conclude that a one year increase in the median age of
housing units is associated with a decrease of 0.01104 eviction cases
per 100 renting households. The results also show that the coefficient
has a p-value of 0.00975, which indicates that the <em>medage</em>
coefficient is statistically significant at the 0.01 level. This means
that the probability that the association between <em>medage</em> and
<em>evict</em> is due to chance is 100*0.00975 = 0.975 percent. In other
words, the probability of seeing the association 0.01104 just by chance
if the null hypothesis is true is a little less than one percent.</p>
<p>But, when you include median gross rent, you get</p>
<pre class="r"><code>summary(lm(evict ~ medage + rent, 
           data = sac_metro))</code></pre>
<pre><code>## 
## Call:
## lm(formula = evict ~ medage + rent, data = sac_metro)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8670 -0.8370 -0.2701  0.5416  6.3404 
## 
## Coefficients:
##               Estimate Std. Error t value       Pr(&gt;|t|)    
## (Intercept)  2.6975817  0.4226376   6.383 0.000000000487 ***
## medage       0.0008342  0.0048798   0.171          0.864    
## rent        -0.0009009  0.0002236  -4.028 0.000067339161 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.224 on 396 degrees of freedom
## Multiple R-squared:  0.05541,    Adjusted R-squared:  0.05064 
## F-statistic: 11.62 on 2 and 396 DF,  p-value: 0.00001253</code></pre>
<p><br></p>
<p class="comment">
<strong>Question 3</strong>: Write the equation of the regression line
for the model above. What is the interpretation of the slope coefficient
for <em>rent</em>? What about the slope coefficient for the intercept?
</p>
<p class="comment">
<strong>Question 4</strong>: In your own words, explain why the
statistically significant relationship between median age of housing
units and eviction rates disappeared when we included median rent.
</p>
<p><br></p>
<p>This example illustrates the importance of accounting for potential
confounding in your model. This includes confounding introduced by
spatial dependency or autocorrelation, which we will discuss later in
the quarter.</p>
<div style="margin-bottom:25px;">

</div>
</div>
</div>
<div id="multicollinearity" class="section level2">
<h2><strong>Multicollinearity</strong></h2>
<p><br />
</p>
<p>It might seem that if confounding is such a big problem (and it is
when trying to make causal inferences) you should aim to try to control
for <em>everything.</em> Including the kitchen sink. The downside of
this strategy is that including too many variables will likely introduce
multicollinearity in your model. Multicollinearity is defined to be
high, but not perfect, correlation between two independent variables in
a regression.</p>
<p>What are the effects of multicollinearity? Mainly you will get blown
up standard errors for the coefficient on one of your correlated
variables. In other words, you will not detect a relationship even if
one does exist because the standard error on the coefficient is
artificially inflated.</p>
<p>What to do? First, run a correlation matrix for all your proposed
independent variables. Let’s say we wanted to run an OLS model including
<em>pblk</em>, <em>phisp</em>, <em>pund18</em>, <em>totp</em>,
<em>punemp</em>, <em>prent</em> and <em>pburden</em> as independent
variables. One way of obtaining a correlation matrix is to use the
<code>cor()</code> function. We use the function <code>select()</code>
to keep the variables we need from <em>sac_metro</em>. We use the
<code>round()</code> function to round up the correlation values to two
significant digits after the decimal point.</p>
<pre class="r"><code>round(cor(dplyr::select(sac_metro, pblk, phisp, pund18, punemp, totp, rent, vacancy, medage, pburden)),2)</code></pre>
<pre><code>##          pblk phisp pund18 punemp  totp  rent vacancy medage pburden
## pblk     1.00  0.29   0.30   0.39  0.06 -0.26   -0.09   0.01    0.25
## phisp    0.29  1.00   0.36   0.34  0.07 -0.38   -0.01   0.26    0.24
## pund18   0.30  0.36   1.00   0.13  0.36  0.15   -0.24  -0.25    0.13
## punemp   0.39  0.34   0.13   1.00 -0.03 -0.45    0.12   0.26    0.32
## totp     0.06  0.07   0.36  -0.03  1.00  0.21   -0.25  -0.35    0.00
## rent    -0.26 -0.38   0.15  -0.45  0.21  1.00   -0.17  -0.52   -0.25
## vacancy -0.09 -0.01  -0.24   0.12 -0.25 -0.17    1.00   0.17    0.04
## medage   0.01  0.26  -0.25   0.26 -0.35 -0.52    0.17   1.00    0.08
## pburden  0.25  0.24   0.13   0.32  0.00 -0.25    0.04   0.08    1.00</code></pre>
<p>Any correlation that is high is worth flagging. In this case, we see
a few pairwise correlations that are close to 0.5 that might be worth
keeping in mind.</p>
<p>You can also run your regression and then detect multicollinearity in
your results. Signs of multicollinearity include</p>
<ul>
<li>Large changes in the estimated regression coefficients when a
predictor variable is added or deleted</li>
<li>Lack of statistical significance despite high <span
class="math inline"><em>R</em><sup>2</sup></span></li>
<li>Estimated regression coefficients have an opposite sign from
predicted</li>
</ul>
<p>A formal and likely the most common indicator of multicollinearity is
the Variance Inflation Factor (VIF). Use the function <code>vif()</code>
in the <strong>car</strong> package to get the VIFs for each variable.
Let’s check the VIFs for the proposed model. First, run the model and
save it into <em>lm2</em>.</p>
<pre class="r"><code>lm2 &lt;- lm(evict ~  pblk + phisp + pund18 + punemp + totp + rent+ vacancy + 
            medage + pburden, 
          data = sac_metro)</code></pre>
<p>Then get the VIF values using <code>vif()</code>. As described in the
handout, another measure of multicollinearity - tolerance - can be
obtained from the VIF values.</p>
<p><br></p>
<p class="comment">
<strong>Question 5</strong>: Assess the presence of multicollinearity
using the VIF.
</p>
<p><br></p>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="goodness-of-fit" class="section level2">
<h2><strong>Goodness of fit</strong></h2>
<p><br />
</p>
<p>We can measure the overall fit of a multiple linear regression model
by looking at the multiple <span
class="math inline"><em>R</em><sup>2</sup></span>. The multiple <span
class="math inline"><em>R</em><sup>2</sup></span> is the square of the
correlation between the observed values of Y and the values of Y
predicted by the multiple regression model. Therefore, large values of
multiple <span class="math inline"><em>R</em><sup>2</sup></span>
represent a large correlation between the predicted and observed values
of the outcome. A multiple <span
class="math inline"><em>R</em><sup>2</sup></span> of 1 represents a
situation in which the model perfectly predicts the observed data. As
such, multiple <span class="math inline"><em>R</em><sup>2</sup></span>
is a gauge of how well the model predicts the observed data. It follows
that the resulting <span
class="math inline"><em>R</em><sup>2</sup></span> can be interpreted in
the same way as in simple regression: it is the amount of variation in
the outcome variable that is accounted for by the model.</p>
<p><span class="math inline"><em>R</em><sup>2</sup></span> is located in
the summary of the regression model. Let’s run a regression of eviction
rates on percent black and percent unemployment and save the results in
an object called <em>lm3</em>.</p>
<pre class="r"><code>lm3 &lt;- lm(evict ~  pblk + punemp, 
          data = sac_metro)
summary(lm3)          </code></pre>
<pre><code>## 
## Call:
## lm(formula = evict ~ pblk + punemp, data = sac_metro)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.2391 -0.8145 -0.2570  0.4773  6.3674 
## 
## Coefficients:
##             Estimate Std. Error t value         Pr(&gt;|t|)    
## (Intercept) 0.868425   0.120557   7.203 0.00000000000298 ***
## pblk        0.040129   0.008869   4.525 0.00000801258690 ***
## punemp      0.063966   0.015511   4.124 0.00004540918966 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.172 on 396 degrees of freedom
## Multiple R-squared:  0.1344, Adjusted R-squared:   0.13 
## F-statistic: 30.75 on 2 and 396 DF,  p-value: 0.0000000000003863</code></pre>
<p>Let’s compare the model fit to the model we ran earlier that
contained more variables</p>
<pre class="r"><code>summary(lm2) </code></pre>
<pre><code>## 
## Call:
## lm(formula = evict ~ pblk + phisp + pund18 + punemp + totp + 
##     rent + vacancy + medage + pburden, data = sac_metro)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4079 -0.7978 -0.2394  0.5322  6.0918 
## 
## Coefficients:
##                Estimate  Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.27265022  0.55042153   0.495 0.620634    
## pblk         0.03043669  0.00932736   3.263 0.001199 ** 
## phisp       -0.00301827  0.00581229  -0.519 0.603854    
## pund18       4.60136955  1.19358359   3.855 0.000135 ***
## punemp       0.03396615  0.01716201   1.979 0.048504 *  
## totp        -0.00002984  0.00003405  -0.876 0.381335    
## rent        -0.00041215  0.00024403  -1.689 0.092027 .  
## vacancy      0.01452696  0.00770857   1.885 0.060240 .  
## medage       0.00605126  0.00495252   1.222 0.222502    
## pburden      0.00801516  0.00578321   1.386 0.166561    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.146 on 389 degrees of freedom
## Multiple R-squared:  0.1872, Adjusted R-squared:  0.1683 
## F-statistic: 9.952 on 9 and 389 DF,  p-value: 0.00000000000008843</code></pre>
<p><br></p>
<p class="comment">
<strong>Question 6</strong>: Which model yields a better fit, lm3 or
lm2? Explain why.
</p>
<p class="comment">
<strong>Question 7</strong>: For the model you picked in Question 6, run
diagnostics to determine whether it meets the OLS assumptions of
normally distributed errors, homoscedasticity, and linearity. Summarize
the results.
</p>
<hr />
<p><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This
work is licensed under a
<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative
Commons Attribution-NonCommercial 4.0 International License</a>.</p>
<p>Website created and maintained by <a
href="https://nbrazil.faculty.ucdavis.edu/">Noli Brazil</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>


</body>
</html>
