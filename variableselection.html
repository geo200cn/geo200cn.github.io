<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Professor Noli Brazil" />

<meta name="date" content="2024-05-20" />

<title>Variable Selection</title>

<script src="site_libs/header-attrs-2.22/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
      .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">GEO 200CN: Spring 2024</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
<li>
  <a href="hw_guidelines.html">Assignment Guidelines</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="eda.html">Exploratory Data Analysis</a>
    </li>
    <li>
      <a href="inference.html">Statistical Inference</a>
    </li>
    <li>
      <a href="hypothesis.html">Hypothesis Testing</a>
    </li>
    <li>
      <a href="linearregression.html">Linear Regression</a>
    </li>
    <li>
      <a href="linearregression2.html">More Linear Regression</a>
    </li>
    <li>
      <a href="logistic.html">Logistic Regression</a>
    </li>
    <li>
      <a href="introspatial.html">Intro to Spatial Data</a>
    </li>
    <li>
      <a href="pointpatterns.html">Point Pattern Analysis</a>
    </li>
    <li>
      <a href="spatialautocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="spatialreg.html">Spatial Regression</a>
    </li>
    <li>
      <a href="prediction.html">Prediction Modelling</a>
    </li>
    <li>
      <a href="variableselection.html">Variable Selection</a>
    </li>
    <li>
      <a href="interpolation.html">Spatial Interpolation</a>
    </li>
    <li>
      <a href="kriging.html">Kriging</a>
    </li>
    <li>
      <a href="regtrees.html">Regression Trees</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Other
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tips.html">R Tips</a>
    </li>
    <li>
      <a href="data_wrangling.html">Data Wrangling</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Variable Selection</h1>
<h3 class="subtitle"><font size="4">GEO 200CN - Quantitative
Geography</font></h3>
<h4 class="author">Professor Noli Brazil</h4>
<h4 class="date">May 20, 2024</h4>

</div>


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: normal;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>
<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>
<p><br />
</p>
<p>In choosing a predictive model, you might want to run different types
of models - say, logistic vs. linear regression - and use the cross
validation techniques we learned from last lab to choose the model that
provides the best predictions. Rather than choosing between different
models of varying specification, suppose that we have available a set of
variables to predict an outcome of interest and want to choose a subset
of those variables that accurately predict the outcome. That is, you
have decided on the type of model, and want to select the set of
variables among a large pool that provides the best prediction. The
driving force behind variable selection is the desire for a parsimonious
regression model (one that is simpler and easier to interpret) and the
need for greater accuracy in prediction. In this lab guide, we will go
through variable selection methods.</p>
<p>The objectives of this lab are as follows</p>
<ol style="list-style-type: decimal">
<li>Learn how to run forward and backward stepwise selection</li>
<li>Learn how to run ridge and lasso regression</li>
</ol>
<p>To help us accomplish these objectives, we will use a data set from
the 2017 <a href="https://www.cdc.gov/brfss/index.html">Behavioral Risk
Factor Surveillance System</a> (BRFSS), an annual survey conducted by
the Centers for Disease Control and Prevention that collects state data
about U.S. residents regarding their health-related risk behaviors,
chronic health conditions, and use of preventive services, to examine
the association between individual health status and a set of
demographic and socioeconomic characteristics. The lab follows closely
this week’s readings in ISLR.</p>
<div style="margin-bottom:25px;">

</div>
<div id="installing-and-loading-packages" class="section level2">
<h2><strong>Installing and loading packages</strong></h2>
<p><br />
</p>
<p>We’ll be using a few new packages in this lab. First, install them if
you haven’t already.</p>
<pre class="r"><code>install.packages(c(&quot;leaps&quot;, &quot;glmnet&quot;))</code></pre>
<p>Second, load all necessary packages.</p>
<pre class="r"><code>library(broom)
library(dismo)
library(boot)
library(tidyverse)
library(leaps)
library(glmnet)</code></pre>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="bring-in-the-data" class="section level2">
<h2><strong>Bring in the data</strong></h2>
<p><br />
</p>
<p>Download the data for this lab guide from Canvas in the Week 8 Lab
and Assignment folder. Bring in the BRFSS file <em>brfss17.csv</em> file
into R.</p>
<pre class="r"><code>brfss17 &lt;- read_csv(&quot;brfss17.csv&quot;)</code></pre>
<p>The data contain individuals as units of observations. Here, we’ll
predict the numeric variable <em>physhlth</em>, which gives the number
of days in the past month that respondents felt that they were not in
good physical health. Our independent variables are age <em>agec</em>,
gender <em>male</em>, educational attainment <em>educ</em>,
race/ethnicity <em>race_eth</em>, whether the individual indicates they
<em>smoke</em>, employment status <em>employ</em>, marital status
<em>marst</em>, BMI <em>bmi</em>, health insurance <em>ins</em>, income
<em>inc</em>, and indicators of whether the individual has high blood
pressure <em>bphigh</em>, high cholesterol <em>toldhi</em>, and was ever
diagnosed with a heart attack <em>cvdinfr</em>, a Stroke
<em>cvdstrk</em>, Asthma <em>asthma</em>, Arthritis <em>havarth</em>, or
Diabetes <em>diabete</em>. A record layout of the data can be found <a
href="https://raw.githubusercontent.com/geo200cn/data/master/brfss17RL.txt">here</a>.</p>
<p>Our research question is: What set of variables provide the best
predictive quality of a person’s physical health?</p>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="subset-selection" class="section level2">
<h2><strong>Subset Selection</strong></h2>
<p><br />
</p>
<p>Subset selection refers to the task of finding a small subset of the
available independent variables that does a good job of predicting the
dependent variable. We then fit a statistical model using just the
reduced set of variables.</p>
<div style="margin-bottom:25px;">

</div>
<div id="forward-and-backward-stepwise-selection"
class="section level3">
<h3><strong>Forward and Backward Stepwise Selection</strong></h3>
<p><br />
</p>
<p>The purpose of forward and backward stepwise selection is to
iteratively fit a regression model by adding predictors one-by-one to a
model with no variables (known as a null model) or subtracting
predictors one-by-one from a full model (all predictors are included).
The goal is to find the best combination of predictors, where best is
based on some measure of predictive quality.</p>
<p>We can use the <code>regsubsets()</code> function, which is a part of
the <strong>leaps</strong> package, to perform the best subset selection
methods described in ISLR Ch 6.1. Let’s first run a forward stepwise
selection (see ISLR 6.1.2) to find the best predictors for poor physical
health days. The syntax is the same as <code>lm()</code> and
<code>glm()</code>, but we add the argument
<code>method = "forward"</code> to indicate forward stepwise
selection.</p>
<pre class="r"><code>regfit1.fwd &lt;- regsubsets(physhlth ~ bmi + race_eth + agec + male + smoke + educ + 
                            inc + employ + marst + ins + bphigh + toldhi + 
                            cvdinfr + cvdstrk + asthma + havarth + diabete, 
                          data = brfss17, 
                          method = &quot;forward&quot;)</code></pre>
<p>A summary of results.</p>
<pre class="r"><code>summary(regfit1.fwd)</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(physhlth ~ bmi + race_eth + agec + male + 
##     smoke + educ + inc + employ + marst + ins + bphigh + toldhi + 
##     cvdinfr + cvdstrk + asthma + havarth + diabete, data = brfss17, 
##     method = &quot;forward&quot;)
## 33 Variables  (and intercept)
##                      Forced in Forced out
## bmi                      FALSE      FALSE
## race_ethnh black         FALSE      FALSE
## race_ethnh multirace     FALSE      FALSE
## race_ethnh other         FALSE      FALSE
## race_ethnhwhite          FALSE      FALSE
## agec(24,39]              FALSE      FALSE
## agec(39,59]              FALSE      FALSE
## agec(59,79]              FALSE      FALSE
## agec(79,99]              FALSE      FALSE
## maleMale                 FALSE      FALSE
## smokeFormer              FALSE      FALSE
## smokeNeverSmoked         FALSE      FALSE
## educ1somehs              FALSE      FALSE
## educ2hsgrad              FALSE      FALSE
## educ3somecol             FALSE      FALSE
## educ4colgrad             FALSE      FALSE
## inc                      FALSE      FALSE
## employnilf               FALSE      FALSE
## employretired            FALSE      FALSE
## employunable             FALSE      FALSE
## marstdivorced            FALSE      FALSE
## marstmarried             FALSE      FALSE
## marstnm                  FALSE      FALSE
## marstseparated           FALSE      FALSE
## marstwidowed             FALSE      FALSE
## ins                      FALSE      FALSE
## bphigh                   FALSE      FALSE
## toldhi                   FALSE      FALSE
## cvdinfr                  FALSE      FALSE
## cvdstrk                  FALSE      FALSE
## asthma                   FALSE      FALSE
## havarth                  FALSE      FALSE
## diabete                  FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: forward
##          bmi race_ethnh black race_ethnh multirace race_ethnh other
## 1  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 2  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 3  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 4  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 5  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 6  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 7  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
## 8  ( 1 ) &quot; &quot; &quot; &quot;              &quot; &quot;                  &quot; &quot;             
##          race_ethnhwhite agec(24,39] agec(39,59] agec(59,79] agec(79,99]
## 1  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 2  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 3  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 4  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 5  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 6  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 7  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
## 8  ( 1 ) &quot; &quot;             &quot; &quot;         &quot; &quot;         &quot; &quot;         &quot; &quot;        
##          maleMale smokeFormer smokeNeverSmoked educ1somehs educ2hsgrad
## 1  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 2  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 3  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 4  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 5  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 6  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 7  ( 1 ) &quot; &quot;      &quot; &quot;         &quot; &quot;              &quot; &quot;         &quot; &quot;        
## 8  ( 1 ) &quot; &quot;      &quot; &quot;         &quot;*&quot;              &quot; &quot;         &quot; &quot;        
##          educ3somecol educ4colgrad inc employnilf employretired employunable
## 1  ( 1 ) &quot; &quot;          &quot; &quot;          &quot; &quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 2  ( 1 ) &quot; &quot;          &quot; &quot;          &quot; &quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 3  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 4  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 5  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 6  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 7  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
## 8  ( 1 ) &quot; &quot;          &quot; &quot;          &quot;*&quot; &quot; &quot;        &quot; &quot;           &quot;*&quot;         
##          marstdivorced marstmarried marstnm marstseparated marstwidowed ins
## 1  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 2  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 3  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 4  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 5  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 6  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 7  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
## 8  ( 1 ) &quot; &quot;           &quot; &quot;          &quot; &quot;     &quot; &quot;            &quot; &quot;          &quot; &quot;
##          bphigh toldhi cvdinfr cvdstrk asthma havarth diabete
## 1  ( 1 ) &quot; &quot;    &quot; &quot;    &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;     &quot; &quot;    
## 2  ( 1 ) &quot; &quot;    &quot; &quot;    &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot;*&quot;     &quot; &quot;    
## 3  ( 1 ) &quot; &quot;    &quot; &quot;    &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot;*&quot;     &quot; &quot;    
## 4  ( 1 ) &quot; &quot;    &quot; &quot;    &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot;*&quot;     &quot;*&quot;    
## 5  ( 1 ) &quot; &quot;    &quot; &quot;    &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot;*&quot;     &quot;*&quot;    
## 6  ( 1 ) &quot; &quot;    &quot; &quot;    &quot;*&quot;     &quot; &quot;     &quot;*&quot;    &quot;*&quot;     &quot;*&quot;    
## 7  ( 1 ) &quot; &quot;    &quot; &quot;    &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;     &quot;*&quot;    
## 8  ( 1 ) &quot; &quot;    &quot; &quot;    &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;     &quot;*&quot;</code></pre>
<p>The <code>regsubsets()</code> function identifies the best model that
contains a given number of predictors, where <em>best</em> is quantified
as the lowest residual sum of squares (RSS). An asterisk indicates that
a given variable is included in the corresponding best model. For
example, the summary output indicates that the best two-variable model
contains <em>employunable</em> and <em>havarth</em>. When interpreting
these results, remember that categorical/qualitative variables are
separated into dummy variables. Therefore, the variables
<em>employnilf</em>, <em>employretired</em> and <em>employunable</em>
correspond to separate dummies for the categories “not in labor force”,
“retired”, and “unable to work” (the category “Employed” is the
reference) in the <em>employ</em> variable.</p>
<p>You can extract the coefficients and their values for the best
j-variable model by using the <code>coef()</code> function. For example,
to get the coefficients for the best 2-variable model, type in:</p>
<pre class="r"><code>coef(regfit1.fwd, 2)</code></pre>
<pre><code>##  (Intercept) employunable      havarth 
##     2.141179    11.496099     3.955133</code></pre>
<p>By default, the function only reports results up to the best
eight-variable model, but you can use the <code>nvmax</code> option to
return as many variables as desired. For example, let’s try a 33
variable model, with 33 representing the max number of predictors we can
include in a model.</p>
<pre class="r"><code>regfit2.fwd &lt;- regsubsets(physhlth ~ bmi + race_eth + agec + male + smoke + educ + 
                            inc + employ + marst + ins + bphigh + toldhi +
                            cvdinfr + cvdstrk + asthma + havarth + diabete, 
                          data = brfss17, 
                          method = &quot;forward&quot;, 
                          nvmax = 33)</code></pre>
<p>After getting the best model for each given number of predictors, we
need to then select the best model across all sets 1 through
<code>nvmax</code>. Following ISLR, we can examine the adjusted <span
class="math inline"><em>R</em><sup>2</sup></span>, <span
class="math inline"><em>C</em><em>p</em></span> and the Bayesian
Information Criterion (BIC) from the <code>summary()</code> output.
Let’s examine the adjusted <span
class="math inline"><em>R</em><sup>2</sup></span>, which is discussed on
page 212 in ISLR. Let’s save a summary of the model in an object, and
then extract the adjusted <span
class="math inline"><em>R</em><sup>2</sup></span> use base R
convention.</p>
<pre class="r"><code>regfit2.fwd.summary &lt;- summary(regfit2.fwd)
regfit2.fwd.summary$adjr2</code></pre>
<pre><code>##  [1] 0.1277702 0.1732549 0.1872198 0.1944227 0.1997421 0.2040115 0.2068957
##  [8] 0.2081920 0.2093504 0.2105600 0.2117409 0.2125584 0.2132344 0.2138314
## [15] 0.2143430 0.2146413 0.2148545 0.2150567 0.2151948 0.2152931 0.2153752
## [22] 0.2154121 0.2154404 0.2154770 0.2155044 0.2155138 0.2155498 0.2155559
## [29] 0.2155538 0.2155509 0.2155471 0.2155545 0.2155499</code></pre>
<p>We see that the adjusted <span
class="math inline"><em>R</em><sup>2</sup></span> increases from 12.8
with one variable to 21.6 with all variables.</p>
<p>We can look at all fit indicators to decide which model is the best.
Plotting adjusted <span
class="math inline"><em>R</em><sup>2</sup></span>, <span
class="math inline"><em>C</em><em>p</em></span>, and BIC for all of the
models at once can help us decide which model to select. We’ll use the
basic <code>plot()</code> function here, and note the
<code>type="l"</code> option tells R to connect the plotted points with
lines.</p>
<pre class="r"><code>par(mfrow = c(2,2))

plot(regfit2.fwd.summary$adjr2 , 
     xlab =&quot; Number of Variables &quot;, 
     ylab =&quot; Adjusted RSq &quot;, type =&quot;l&quot;)

plot(regfit2.fwd.summary$cp, 
     xlab =&quot; Number of Variables &quot;, 
     ylab =&quot;Cp&quot;, 
     type =&#39;l&#39;)

plot(regfit2.fwd.summary$bic ,
     xlab =&quot; Number of Variables &quot;,
     ylab =&quot; BIC &quot;, 
     type =&#39;l&#39;)</code></pre>
<p><img
src="variableselection_files/figure-html/unnamed-chunk-9-1.png" /><!-- --></p>
<p><br></p>
<p class="comment">
<strong>Question 1</strong>: Which model does the adjusted <span
class="math inline"><em>R</em><sup>2</sup></span> indicate is the best?
In your answer, identify the specific variables that are included in the
best model.
</p>
<p class="comment">
<strong>Question 2</strong>: Run backwards stepwise selection with nvmax
= 33. Which model is identified as the best model based on adjusted
<span class="math inline"><em>R</em><sup>2</sup></span>? In your answer,
identify the specific variables that are included in the best model.
</p>
<p><br></p>
<div style="margin-bottom:25px;">

</div>
</div>
<div id="cross-validation-error" class="section level3">
<h3><strong>Cross-validation error</strong></h3>
<p><br />
</p>
<p>We just saw that it is possible to choose among a set of models of
different sizes using BIC, adjusted <span
class="math inline"><em>R</em><sup>2</sup></span> and other best fit
metrics. We will now consider how to do this within a cross-validation
framework. Here, you will use cross-validation to find the best model
with the lowest validation error. This will combine the methods from
ISLR Chapter 5 and Chapter 6, which is described on page 213 in
ISLR.</p>
<p>We will choose among the best models of different sizes using cross
validation. This approach is somewhat involved, as we must perform
subset selection within each of the <em>k</em> training sets. Despite
this, subsetting in R makes this job quite easy. First, we create a
vector that allocates each observation to one of <em>k</em> = 10 folds
using the fabulous function <code>kfold()</code>. We set the seed for
reproducability.</p>
<pre class="r"><code>set.seed(1234)
brfss17 &lt;- brfss17 %&gt;%
            mutate(folds = kfold(brfss17, k = 10))
table(brfss17$folds)</code></pre>
<pre><code>## 
##     1     2     3     4     5     6     7     8     9    10 
## 15811 15810 15811 15811 15811 15810 15811 15811 15810 15811</code></pre>
<p>and we create an empty matrix named <em>cv.errors</em> in which we
will store the results. Rows represent the k = 10 folds and columns
represent the complete set of variables (33) we are including in the
model (so first column is for the 1-variable model, second is the
2-variable model and so on).</p>
<pre class="r"><code>k=10
cv.errors = matrix(NA ,k,33, dimnames =list(NULL , paste (1:33) ))
cv.errors </code></pre>
<pre><code>##        1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
##  [1,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [2,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [3,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [4,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [5,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [6,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [7,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [8,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##  [9,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
## [10,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
##       25 26 27 28 29 30 31 32 33
##  [1,] NA NA NA NA NA NA NA NA NA
##  [2,] NA NA NA NA NA NA NA NA NA
##  [3,] NA NA NA NA NA NA NA NA NA
##  [4,] NA NA NA NA NA NA NA NA NA
##  [5,] NA NA NA NA NA NA NA NA NA
##  [6,] NA NA NA NA NA NA NA NA NA
##  [7,] NA NA NA NA NA NA NA NA NA
##  [8,] NA NA NA NA NA NA NA NA NA
##  [9,] NA NA NA NA NA NA NA NA NA
## [10,] NA NA NA NA NA NA NA NA NA</code></pre>
<p>Now we write a for loop that performs cross-validation. The loop is
similar to the one we used to do cross-validation on California
temperature in the <a
href="https://geo200cn.github.io/prediction.html#Cross-Validation">last
lab</a>. In the <span class="math inline"><em>k</em></span>th fold, the
elements of folds that equal <em>k</em> are in the test (or validation)
set, and the remainder are in the training set. We make our predictions
for each model size, compute the test errors on the appropriate subset,
and store them in the appropriate slot in the matrix
<em>cv.errors</em>.</p>
<p>There is a complication: there is no <code>predict()</code> method
for <code>regsubsets()</code> like there is for <code>glm()</code> or
<code>lm()</code>. Fortunately, ISLR created a predict function for us
to use for <code>regsubsets()</code> objects. I copy and paste their
code below. If you would like to learn how this function was created,
see ISLR 6.5.3.</p>
<pre class="r"><code>predict.regsubsets = function(object ,newdata ,id ,...) {
 form =as.formula(object$call[[2]])
 mat = model.matrix(form, newdata )
 coefi =coef(object ,id=id)
 xvars = names(coefi)
 mat [, xvars ]%*%coefi
}</code></pre>
<p>Now the for loop. Remember, for each fold k, we need to run the
forward stepwise regression, and then for each best model of 1, 2, … 33
variables, we make the prediction. So, we have a 10 (fold) by 33
(models) matrix of error rates.</p>
<pre class="r"><code>for (j in 1:k){
  best.fit = regsubsets(physhlth ~ bmi + race_eth + agec + male + smoke + 
                          educ + inc + employ + marst + ins + bphigh + 
                          toldhi + cvdinfr + cvdstrk + asthma + havarth + 
                          diabete,
                        data = filter(brfss17, folds != j), 
                        method = &quot;forward&quot;, 
                        nvmax = 33)
  for (i in 1:33) {
    pred= predict.regsubsets(best.fit , filter(brfss17, folds == j), id=i)
    pred2 &lt;- (dplyr::select(filter(brfss17, folds == j), physhlth) - pred)^2
    cv.errors[j,i]= mean(pred2$physhlth)
    }
}</code></pre>
<p>This has given us a 10×33 matrix, of which the <em>(i, j)</em>th
element corresponds to the test MSE for the ith cross-validation fold
for the best <em>j</em>-variable model. We use the <code>apply()</code>
function to average over the columns of this matrix in order to obtain a
vector for which the <em>j</em>th element is the cross-validation error
for the <em>j</em>-variable model (see more about the apply function <a
href="https://rspatial.org/intr/9-apply.html">here</a>).</p>
<pre class="r"><code>mean.cv.errors =apply(cv.errors ,2, mean)
mean.cv.errors</code></pre>
<pre><code>##        1        2        3        4        5        6        7        8 
## 63.17279 59.87841 58.86750 58.34605 57.96128 57.65206 57.44342 57.37276 
##        9       10       11       12       13       14       15       16 
## 57.29870 57.19102 57.12315 57.04165 56.98713 56.95636 56.90768 56.88672 
##       17       18       19       20       21       22       23       24 
## 56.87541 56.86083 56.84728 56.84648 56.83535 56.84263 56.84084 56.83513 
##       25       26       27       28       29       30       31       32 
## 56.83088 56.83287 56.82971 56.82791 56.82835 56.82959 56.83118 56.83100 
##       33 
## 56.83043</code></pre>
<p>The model with the lowest error rate is</p>
<pre class="r"><code>min(mean.cv.errors)</code></pre>
<pre><code>## [1] 56.82791</code></pre>
<p>which corresponds to the 28-variable model. Let’s plot to
visualize</p>
<pre class="r"><code>par(mfrow =c(1 ,1))
plot( mean.cv.errors , type=&#39;b&#39;)</code></pre>
<p><img
src="variableselection_files/figure-html/unnamed-chunk-16-1.png" /><!-- --></p>
<p><br></p>
<p class="comment">
<strong>Question 3</strong>: What are the variables and their
coefficient values associated with the 28-variable model?
</p>
<p><br></p>
<div style="margin-bottom:25px;">

</div>
</div>
</div>
<div id="shrinkage-methods" class="section level2">
<h2><strong>Shrinkage Methods</strong></h2>
<p><br />
</p>
<p>The basis behind shrinkage methods is to shrink the coefficients
towards 0. Why would shrunk coefficients be better? This introduces
bias, but may significantly decrease the variance of the estimates. If
the latter effect is larger, this would decrease the test error. The
driving force behind variable selection: the need for greater accuracy
in prediction. In a prediction context, there is less concern about the
values of the components on the right-hand side, rather interest is on
the total contribution. We’ll cover two shrinkage methods: ridge and
lasso regression. If you want to dig deep into the math, check ISLR Ch.
6.2. Otherwise, follow the conceptual pathway rather than getting too
deep (or lost) in the weeds.</p>
<div style="margin-bottom:25px;">

</div>
<div id="ridge-and-lasso-regression" class="section level3">
<h3><strong>Ridge and Lasso Regression</strong></h3>
<p><br />
</p>
<p>We can fit ridge and lasso regression models using the function
<code>glmnet()</code> which is a part of the <strong>glmnet</strong>
package. This function has slightly different syntax from other
model-fitting functions that we have encountered thus far in this class
(e.g. <code>lm()</code>, <code>glm()</code>, <code>lagsarlm()</code>).
In particular, we must pass in a matrix <em>x</em> of independent
variables rather than a data frame as well as a <em>y</em> vector.</p>
<p>The <code>model.matrix()</code> function is particularly useful for
creating <em>x</em>; not only does it produce a matrix corresponding to
the 33 predictors but it also automatically transforms any qualitative
variables into dummy variables. The latter property is important because
<code>glmnet()</code> can only take numerical, quantitative inputs.
Create <em>x</em> using <code>model.matrix()</code> and set <em>y</em>
as the response variable <em>brfss17$physhlth</em>.</p>
<pre class="r"><code>x &lt;- model.matrix(physhlth ~., dplyr::select(brfss17, physhlth, bmi, race_eth, 
                                             agec, male, smoke , educ, inc, employ, 
                                             marst, ins, bphigh, toldhi, cvdinfr, 
                                             cvdstrk, asthma, havarth, diabete))[, -1]
y &lt;- brfss17$physhlth</code></pre>
<p>We’ve got our pieces to plug into <code>glmnet()</code>. The function
has an alpha argument that determines what type of model is fit. If
<code>alpha=0</code> then a ridge regression model is fit, and if
<code>alpha=1</code> then a lasso model is fit. We also need to specify
the argument <code>lambda</code>.</p>
<pre class="r"><code>grid &lt;-10^seq(10,-2, length =100)
regfit.ridge = glmnet(x,y,alpha =0, lambda =grid )</code></pre>
<p>Recall from ISLR 6.2.3, lambda is our key tuning parameter. By
default the <code>glmnet()</code> function performs ridge regression for
an automatically selected range of <code>lambda</code> values. However,
here we have chosen to implement the function over a grid of 100 values
ranging from <code>lambda</code> = <span
class="math inline">10<sup>10</sup></span> to <code>lambda</code> =
<span class="math inline">10<sup>−2</sup></span>. As we will see, we can
also compute model fits for a particular value of lambda that is not one
of the original grid values. Note that by default, the
<code>glmnet()</code> function standardizes the variables so that they
are on the same scale. To turn off this default setting, use the
argument <code>standardize = FALSE</code>.</p>
<p>Associated with each value of lambda is a vector of ridge regression
coefficients, stored in a matrix that can be accessed by
<code>coef()</code>. In this case, it is a 34×100 matrix, with 34 rows
(one for each predictor, plus an intercept) and 100 columns (one for
each value of lambda).</p>
<pre class="r"><code>dim(coef(regfit.ridge))</code></pre>
<pre><code>## [1]  34 100</code></pre>
<p>As ISLR describes, we expect the coefficient estimates to be much
smaller when a large value of lambda is used, as compared to when a
small value of lambda is used. These are the coefficients when lambda =
11498</p>
<pre class="r"><code>regfit.ridge$lambda[50]</code></pre>
<pre><code>## [1] 11497.57</code></pre>
<pre class="r"><code>coef(regfit.ridge)[,50]</code></pre>
<pre><code>##          (Intercept)                  bmi     race_ethnh black 
##         4.084313e+00         1.268837e-04         2.626554e-04 
## race_ethnh multirace     race_ethnh other      race_ethnhwhite 
##         1.066839e-03        -3.761764e-04        -2.111790e-04 
##          agec(24,39]          agec(39,59]          agec(59,79] 
##        -1.408315e-03        -4.585996e-06         8.351994e-04 
##          agec(79,99]             maleMale          smokeFormer 
##         1.053666e-03        -6.639874e-04         6.756729e-04 
##     smokeNeverSmoked          educ1somehs          educ2hsgrad 
##        -1.428837e-03         2.575934e-03         9.878735e-04 
##         educ3somecol         educ4colgrad                  inc 
##         5.819408e-04        -1.629427e-03        -1.103627e-03 
##           employnilf        employretired         employunable 
##         3.085549e-04         7.601816e-04         9.529839e-03 
##        marstdivorced         marstmarried              marstnm 
##         1.411200e-03        -1.160753e-03        -3.079830e-04 
##       marstseparated         marstwidowed                  ins 
##         2.114507e-03         1.287844e-03        -2.362022e-05 
##               bphigh               toldhi              cvdinfr 
##         2.071257e-03         1.553035e-03         4.122153e-03 
##              cvdstrk               asthma              havarth 
##         4.690407e-03         2.345151e-03         3.689490e-03 
##              diabete 
##         3.111607e-03</code></pre>
<p>In contrast, here are the coefficients when lambda = 705.</p>
<pre class="r"><code>regfit.ridge$lambda[60]</code></pre>
<pre><code>## [1] 705.4802</code></pre>
<pre class="r"><code>coef(regfit.ridge)[,60]</code></pre>
<pre><code>##          (Intercept)                  bmi     race_ethnh black 
##         4.0504905434         0.0020005999         0.0037397803 
## race_ethnh multirace     race_ethnh other      race_ethnhwhite 
##         0.0169757526        -0.0057351102        -0.0031814796 
##          agec(24,39]          agec(39,59]          agec(59,79] 
##        -0.0218565403         0.0002131867         0.0128217695 
##          agec(79,99]             maleMale          smokeFormer 
##         0.0162648940        -0.0105069817         0.0103408837 
##     smokeNeverSmoked          educ1somehs          educ2hsgrad 
##        -0.0224062522         0.0404244624         0.0153098275 
##         educ3somecol         educ4colgrad                  inc 
##         0.0090557469        -0.0254009094        -0.0174243586 
##           employnilf        employretired         employunable 
##         0.0051264463         0.0116170232         0.1521595719 
##        marstdivorced         marstmarried              marstnm 
##         0.0220844534        -0.0180530037        -0.0048032994 
##       marstseparated         marstwidowed                  ins 
##         0.0333203184         0.0197562125        -0.0003278925 
##               bphigh               toldhi              cvdinfr 
##         0.0324397431         0.0243132571         0.0651214899 
##              cvdstrk               asthma              havarth 
##         0.0741552994         0.0373256610         0.0585609896 
##              diabete 
##         0.0490685846</code></pre>
<p>OK, so which lambda do we go with? In general, instead of arbitrarily
choosing a lambda, it would be better to use our new best pal
cross-validation to choose the value of this tuning parameter. This is
described on page 227 in ISLR. Instead of running a for loop, we can do
this using the built-in cross-validation function,
<code>cv.glmnet()</code>. By default, the function
<code>cv.glmnet()</code> performs ten-fold cross-validation, though this
can be changed using the argument <code>folds</code>. Note that we set a
random seed first so our results will be reproducible, because the
choice of the cross-validation folds is random.</p>
<pre class="r"><code>set.seed(1234)
cv.out =cv.glmnet(x,y, alpha =0, lambda = grid)</code></pre>
<p>We can plot the mean squared error by lambda</p>
<pre class="r"><code>plot(cv.out)</code></pre>
<p><img
src="variableselection_files/figure-html/unnamed-chunk-23-1.png" /><!-- --></p>
<p>But let’s get the lambda that minimizes the error.</p>
<pre class="r"><code>bestlam =cv.out$lambda.min
bestlam</code></pre>
<pre><code>## [1] 0.03053856</code></pre>
<p>We see that the value of lambda that results in the smallest
cross-validation error is 0.03053856.</p>
<p>We’ve got our best lambda, now we fit the ridge regression.</p>
<pre class="r"><code>out &lt;- glmnet(x,y,alpha =0, lambda = bestlam)</code></pre>
<p>The ridge regression coefficients for our new value of lambda is</p>
<pre class="r"><code>coef(out)</code></pre>
<pre><code>## 34 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                s0
## (Intercept)           2.830499147
## bmi                   0.047532909
## race_ethnh black     -0.755931016
## race_ethnh multirace  0.594441633
## race_ethnh other      0.089154005
## race_ethnhwhite       0.198880924
## agec(24,39]           0.446930184
## agec(39,59]           0.535009679
## agec(59,79]          -0.146020858
## agec(79,99]           0.074366529
## maleMale             -0.256979833
## smokeFormer          -0.581880096
## smokeNeverSmoked     -0.948514406
## educ1somehs           0.002260339
## educ2hsgrad          -0.180086678
## educ3somecol         -0.003100523
## educ4colgrad         -0.202204595
## inc                  -0.533779319
## employnilf            1.265765977
## employretired         1.010321146
## employunable          9.768290932
## marstdivorced         0.031915815
## marstmarried         -0.269647318
## marstnm              -0.322224688
## marstseparated        0.339368048
## marstwidowed         -0.494083775
## ins                   0.065772033
## bphigh                0.522395761
## toldhi                0.298632337
## cvdinfr               2.226214922
## cvdstrk               2.276251323
## asthma                1.499765561
## havarth               2.918022579
## diabete               1.488135575</code></pre>
<p>Unlike stepwise regression, which controls the complexity of the
model by restricting the number of predictors, ridge regression keeps
all of the predictor variables in the model, and shrinks the
coefficients toward zero.</p>
<p>Least absolute shrinkage and selection operator (Lasso) performs
variable selection and regularization to increase prediction accuracy of
the model…the only difference to ridge is that the regularization term
is an absolute value. So instead of squaring the slope, which is done in
ridge regression, we take the absolute value. As a result Lasso
regression can increase lambda then set irrelevant parameters to 0
(Ridge will just shrink them but not set to 0) — relevant parameters
might shrink a little bit in both too. The advantage here is that Lasso
can exclude useless variables — so if you have a lot of variables it
might be preferable. However, if most variables are useful then Ridge
regression would probably work better.</p>
<p>The above procedure uses cross-validation to decide which lambda to
choose. But what if you wanted to decide whether ridge regression is
better than our other shrinkage method, the lasso? You can separate your
data set into training and test sets, run each model on the training
set, and then calculate RMSE or MSE using the test sets.</p>
<p><br></p>
<p class="comment">
<strong>Question 4</strong>: Use 10-fold cross validation to find the
best value of the tuning parameter for Lasso regression.
</p>
<p class="comment">
<strong>Question 5</strong>: Using the tuning parameter we identified in
the lab guide for Ridge and the one you identified in Question 4 for
Lasso, use 10-fold cross validation to choose between Ridge and Lasso
regression models based on RMSE? Use the RMSE function you created in
last lab to calculate the RMSE. Hint: You will be using similar code
from last lab to calculate 10 RMSEs for each model, take the mean of the
10 RMSEs, and compare this mean between Ridge and Lasso. Note that
unlike <code>regsubsets()</code>, the regular <code>predict()</code>
function works for <code>glmnet()</code>.
</p>
<hr />
<p><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This
work is licensed under a
<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative
Commons Attribution-NonCommercial 4.0 International License</a>.</p>
<p>Website created and maintained by <a
href="https://nbrazil.faculty.ucdavis.edu/">Noli Brazil</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>


</body>
</html>
