---
title: "Spatial Regression"
subtitle: <h4 style="font-style:normal">GEO 200CN - Quantitative Geography</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    mathjax: local
    self_contained: false
    theme: yeti
    code_folding: show
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```


In this lab guide, we formally incorporate spatial relationships between units of observations in a regression framework.  We will be closely following Chi & Zhu Chapter 4. The objectives of this lab are as follows

1. Learn how to run and interpret a spatial lag model
2. Learn how to run and interpret a spatial error model
3. Learn how to decide between an ordinary least squares, spatial lag, and spatial error model.

To help us accomplish these learning objectives, we will continue examining the association between neighborhood characteristics and COVID-19 case rates in New York City.

<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

We’ll be using one new package in this lab, **spatialreg**, which contains the functions we need to perform spatial regression models in R. First, install the package.
 
```{r warning = FALSE, message = FALSE}
if (!require("spatialreg")) install.packages("spatialreg")
```

Then load it and the other packages we'll be using.

```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(sf)
library(sp)
library(spdep)
library(tmap)
library(spatialreg)
```


<div style="margin-bottom:25px;">
</div>
## **Why run a spatial regression**
\

The reasons why you run a spatial regression significantly overlap with the reasons for [running a regular linear regression](https://geo200cn.github.io/linearregression.html#why_linear_regression).  However, there is one additional important motivation: to explicitly incorporate spatial dependency in the model.  There are two common flavors of spatial regression: the spatial error model (SEM) and the spatial lag model (SLM). The main reason to run a spatial error model is to control for spatial autocorrelation.  We want to do this because spatial autocorrelation breaks the very important assumption that our regression errors are not correlated (Assumption 3 in BBR).  The main reason to run a spatial lag model is to formally model spatial contagion.  We are modelling the impact of our neighbors' outcomes on our own outcome. For example, the impact of nearby crime on neighborhood crime. Or the impact of a plot's crop yield on its neighboring plot's yield. 

So, in one case, we are using spatial regression because we see spatial dependency as a nuisance to control for. In the other case, we are using spatial regression because there is substantive interest in formally examining the impact of neighboring areas.

Our research questions in this lab is: What ecological characteristics are associated with zip code COVID-19 case rates in New York City? Do these relationships change after accounting for unobserved spatial dependency? Do nearby COVID-19 rates influence the rates in a neighborhood?


<div style="margin-bottom:25px;">
</div>
## **Bringing in the data**
\

We will bring in a shape file containing COVID-19 cases per 1,000 residents and demographic and socioeconomic characteristics for New York city zip codes. I zipped up the file and uploaded it onto Github.  Set your working directory to an appropriate folder and use the following code to download and unzip the file.  I also uploaded the file in Canvas in the Lab and Assignments Week 6 folder.

```{r eval = FALSE}
#insert the pathway to the folder you want your data stored into
setwd("insert your pathway here")
#downloads file into your working directory 
download.file(url = "https://raw.githubusercontent.com/geo200cn/data/master/zctanyccovidwk6.zip", destfile = "zctanyccovidwk6.zip")
#unzips the zipped file
unzip(zipfile = "zctanyccovidwk6.zip")
```

```{r warning = FALSE, message = FALSE, echo=FALSE}
download.file(url = "https://raw.githubusercontent.com/geo200cn/data/master/zctanyccovidwk6.zip", destfile = "zctanyccovidwk6.zip")
unzip(zipfile = "zctanyccovidwk6.zip")
```

Bring in the New York City zip code shape file into R using `st_read()`

```{r warning=FALSE, message=FALSE, results = "hide"}
zctanyc <- st_read("zctanyccovidwk6.shp")
```

COVID-19 case data were downloaded from the [NYC Department of Health and Mental Hygiene](https://github.com/nychealth/coronavirus-data) (confirmed cases up through May 1, 2020).  Socioeconomic and demographic data were downloaded from the 2014-2018 [American Community Survey](https://www.census.gov/programs-surveys/acs). A record layout of the data can be found [here](https://raw.githubusercontent.com/geo200cn/data/master/zctanyccovidRL.txt).   

<div style="margin-bottom:25px;">
</div>
## **Basic multiple linear regression**
\

We're interested in examining the zip code characteristics associated with the number of COVID-19 cases per 1,000 residents.  We run a standard multiple linear regression model, which we should always do first before running any type of spatial regression model. Let's run the regression model we left off with in the last lab guide.  Our dependent variable is *covidrate* and our independent variables are percent black *pblk*, percent Hispanic *phisp*, median household income *medincome*, total population *totp*, and percent of residents 65 years old and older *p65old*. Use the function `lm()`.  Save the results in an object named *fit.ols*.

```{r}
#eliminate scientific notation
options(scipen=999)

fit.ols <- lm(covidrate ~  pblk + phisp +  medincome +totp + p65old, data = zctanyc)
```

What are the results? Use `summary()`

```{r}
summary(fit.ols)
```


<div style="margin-bottom:25px;">
</div>
## **Exploratory spatial data analysis**
\

The next step is to conduct an Exploratory Spatial Data Analysis (ESDA) to gain an understanding of how your data are spatially structured. Perhaps there is no spatial dependency present in your data. So, why fancy it up if when you don't need to? 

<div style="margin-bottom:25px;">
</div>
### **Map your data**
\

The first step in ESDA is to map your dependent variable.  Map the COVID-19 case rates


```{r warning = FALSE, message = FALSE}
tm_shape(zctanyc, unit = "mi") +
  tm_polygons(col = "covidrate", style = "quantile",palette = "Blues", 
              border.alpha = 0, title = "") +
  tm_scale_bar(breaks = c(0, 2.5, 5), text.size = 1, position = c("right", "bottom")) +
  tm_layout(main.title = "COVID-19 cases per 1,000 residents by NYC zip codes",  main.title.size = 0.95, frame = FALSE, legend.outside = TRUE)
```


Does it look like our dependent variable exhibits spatial autocorrelation?

Next, you'll want to map the residuals from your OLS regression model. To extract the residuals from *fit.ols*, use the `resid()` function.  Save it back into *zctanyc*.

```{r}
zctanyc <- mutate(zctanyc, olsresid = resid(fit.ols))
```

Next, map the residuals

```{r warning = FALSE, message = FALSE}
tm_shape(zctanyc, unit = "mi") +
  tm_polygons(col = "olsresid", style = "quantile",palette = "Reds", 
              border.alpha = 0, title = "", midpoint = 0) +
  tm_scale_bar(breaks = c(0, 2.5, 5), text.size = 1, position = c("right", "bottom")) +
  tm_layout(main.title = "Residuals from linear regression",  main.title.size = 0.95, frame = FALSE, legend.outside = TRUE)
```

Looks spatially clustered.  Spatial autocorrelation in the residuals breaks BBR assumption X (page X).

<div style="margin-bottom:25px;">
</div>
### **Spatial weights matrix**
\

The exploratory maps show signs of spatial dependency. Rather than eyeballing it, let's formally test it using a measure of spatial autocorrelation.   Before we do so, we need to convert *zctanyc* to an **sp** object because `moran.mc()` only takes in **sp** objects. Use the command `as()`

```{r}
nyc.sp <- as(zctanyc, "Spatial")
```

We then need to create a neighbor object and its associated spatial weights matrix. You need to define

1. Neighbor connectivity (who is you neighbor?)
2. Neighbor weights (how much does your neighbor matter?)


Neighbor relationships in R are represented by neighbor *nb* objects.  An *nb* object identifies the neighbors for each feature in the dataset.  We use the command `poly2nb()` from the **spdep** package to create a contiguity-based neighbor object.  Let's specify Queen connectivity.  

```{r}
nycb<-poly2nb(nyc.sp, queen=T)
```

The other two neighbor types we discussed in the spatial autocorrelation guide include [k-nearest neighbor]() and [distance based neighbor]().

The next step is to assign weights to each neighbor relationship. The weight determines *how much* each neighbor counts.  You will need to employ the `nb2listw()` command. Let's create a weights object *nycw* for our Queen contiguity defined neighbor object *nycb*.  We might have zero neighbor zip codes, so include the `zero.policy = TRUE` argument. 

```{r}
nycw<-nb2listw(nycb, style="W", zero.policy = TRUE)
```

Here, style = "W" indicates that the weights for each spatial unit are standardized to sum to 1 (this is known as row standardization).


<div style="margin-bottom:25px;">
</div>
### **Moran's Scatterplot**
\

We’ve now defined what we mean by neighbor by creating an *nb* object and the influence of each neighbor by creating a spatial weights matrix.  Now we can examine the Moran scatter plot for the dependent variable *covidrate*.

```{r}
moran.plot(nyc.sp$covidrate, listw=nycw, xlab="Standardized COVID-19 Case Rate", ylab="Standardized Lagged COVID-19 Case Rate",
main=c("Moran Scatterplot for COVID-19 case rates", "in New York"), zero.policy = TRUE )
```

Does it look like there is an association? Yes.

<div style="margin-bottom:25px;">
</div>
### **Spatial autocorrelation**
\

The map and Moran scatter plot provide descriptive visualizations of clustering (autocorrelation) in COVID-19 case rates. But, rather than eyeballing the correlation, we can calculate a global index of spatial autocorrelation.

The most popular test of spatial autocorrelation is the Global Moran’s I test. We'll use the command `moran.mc()` in the **spdep** package to calculate the Moran’s I using Monte Carlo simulation to obtain statistical inference. 


```{r}
moran.mc(nyc.sp$covidrate, listw = nycw, nsim=999, zero.policy = TRUE)
```

We should also test spatial autocorrelation in the residuals. Use the `lm.morantest()` function.  The main arguments are your *lm* object and spatial weights object.

```{r}
lm.morantest(fit.ols, listw = nycw, zero.policy=TRUE)
```


<div style="margin-bottom:25px;">
</div>
## **Spatial lag model**
\

Based on the exploratory mapping, Moran scatter plot, and the Moran's I, there appears to be spatial autocorrelation in the dependent variable.  This means that if there is a spatial lag process going on and we fit an OLS model our coefficients will be biased and inefficient.  That is, the coefficient size and sign are not close to their true value and its standard errors are underestimated. This means [trouble](https://www.youtube.com/watch?v=FPzI4dpEcF8). [Big trouble](https://www.youtube.com/watch?v=cGIIhcMCquc). [Real big trouble](https://www.youtube.com/watch?v=AXsBBqPb5YE).

A spatial lag model (SLM) can be estimated in R using the command `lagsarlm()`, which is in the **spatialreg** package.   Fit the SLM

```{r warning = FALSE}
fit.lag<-lagsarlm(covidrate ~  pblk + phisp +  medincome +totp + p65old, data = nyc.sp, listw = nycw, zero.policy=TRUE) 
```

The only difference between the code for `lm()` and `lagsarlm()` is the argument `listw`, which you use to specify the spatial weights matrix.  Also note that `lm()` uses OLS to estimate the model whereas `lagsarlm()` uses Maximum Likelihood Estimation (described on pages 66 and 76 in Chi and Zhu).

Let's calculate the Moran's I on the model's residuals

```{r}
moran.mc(resid(fit.lag), nycw, nsim=999, zero.policy=TRUE)
```

A summary of results

```{r}
summary(fit.lag)
```

The results are formatted in a similar way as the OLS results. The major difference is the inclusion of the results for the spatial lag coefficient labelled "Rho".  R provides different ways to test the statistical significance of Rho depending on the null distribution. Right next to Rho, R shows the LR test value and its associated p-value. Below Rho is the test statistic "z-value" and its associated p-value, which are based on the standard normal distribution. Finally, R shows the Wald statistic and its associated p-value.  To be clear, all are testing the statistical significance of the spatial lag term (null hypothesis is Rho is equal to 0).

<p class="comment", style="font-style:normal">**Question 1**: In your own words, interpret the meaning of the spatial lag coefficient Rho value 0.70864.  </p>

<p class="comment", style="font-style:normal">**Question 2**: What are the differences between the slope coefficient results produced by the SLM and OLS? Focus on differences in statistical significance and effect sizes. What may be some reasons explaining these differences? </p>


<div style="margin-bottom:25px;">
</div>
## **Spatial error model**
\

The spatial error model (SEM) incorporates spatial dependence in the errors. If there is a spatial error process going on and we fit an OLS model our coefficients will be unbiased but inefficient.  That is, the coefficient size and sign are asymptotically correct but its standard errors are underestimated. 

We can estimate a  spatial error model in R using the command `errorsarlm()` also in the **spatialreg** package.

```{r}
fit.err<-errorsarlm(covidrate ~  pblk + phisp +  medincome +totp + p65old, data = nyc.sp, listw = nycw, zero.policy=TRUE) 
```

And the Moran's I of the residuals

```{r}
moran.mc(resid(fit.err), nycw, nsim=999, zero.policy=TRUE)
```


A summary of the model

```{r}
summary(fit.err)
```

<p class="comment", style="font-style:normal">**Question 3**: In your own words, interpret the meaning of the spatial error lag coefficient Lambda value 0.78837. </p>

<p class="comment", style="font-style:normal">**Question 4**: Interpret the coefficient *pblk* for each of the three models, OLS, SLM and SEM. </p>

<div style="margin-bottom:25px;">
</div>
## **Model Selection**
\

We've run all three models, so the question then is which one to choose: OLS, Spatial lag, or Spatial error? You can make this decision a couple of ways. First, rely on comparisons of goodness of fit measures, specifically the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). Second, run formal hypothesis tests comparing the models using Lagrange Multiplier tests.

<div style="margin-bottom:25px;">
</div>
### **AIC and BIC**
\

AIC and BIC rely on the log likelihood, which captures the probability of the observed data given the model parameters. The lower the AIC and BIC, the better model fit.  Rather than rely on one vs another, in general, it might be best to use AIC and BIC together in model selection. 

The `lagsarlm()` and `errorsarlm()` already provide the AIC at the bottom of the summary output.  But, you can call it up by using the `AIC()` function.  Here are the AICs for the OLS, SLM and SEM models.

```{r}
AIC(fit.ols)
AIC(fit.lag)
AIC(fit.err)
```

To get the BIC, use the function `BIC()`

```{r}
BIC(fit.ols)
BIC(fit.lag)
BIC(fit.err)
```

What's the conclusion?

<div style="margin-bottom:25px;">
</div>
### **Lagrange Multiplier Tests**
\

A popular set of tests to determine the appropriate model was proposed by [Anselin (1988)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1538-4632.1988.tb00159.x) (also see [Anselin et al. 1996](https://www.sciencedirect.com/science/article/abs/pii/0166046295021116)),  These tests are elegantly known as Lagrange Multiplier (LM) tests and are discussed by Chi and Zhu throughout Chapter 3. The null in these tests is the OLS model.  A test showing statistical significance rejects this null. 

The LM test uses the likelihood of the models being compared to assess their fit. The likelihood is the probability the data given the parameter estimates.  The goal of a model is to find values for the regression coefficients that maximize the likelihood, that is, to find the set of parameter estimates that make the data most likely. The Lagrange multiplier compares the likelihood of two models.  In our case, you are comparing whether the likelihood of a spatial model (alternative) is statistically higher than that of an OLS (null). We'll use $LM_{lag}$ to refer to a test of SLM vs. OLS and $LM_{err}$ to refer to a test of SEM vs. OLS.

The issue with the Lagrange Multiplier test is when you are comparing the OLS to the spatial lag, you are not taking into account the presence of the spatial error (and vice versa).  In other words, a test of the SLM against the null of an OLS may be rejecting the null in the presence of the spatial error model.  Recognizing this issue, [Anselin et al. (1996)](https://www.sciencedirect.com/science/article/abs/pii/0166046295021116) make an asymptotic adjustment to the LM test to correct for this.  They formulate robust versions of the LM tests - robust to the presence of the other spatial model.  


We will not go into great detail on how these LM tests are derived.  If you are interested in these details, I suggest reading [Anselin (1988)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1538-4632.1988.tb00159.x), which formulates the non robust LM tests, and [Anselin et al. (1996)](https://www.sciencedirect.com/science/article/abs/pii/0166046295021116), which constructs the robust LM tests.


We're still not quite there in terms of how to choose between the three different models.  The LM test tests  between the OLS and SLM and the OLS and SEM, but cannot directly compare SLM and SEM.  The SLM and SEM  are non-nested models (the OLS is a nested model of SLM - take out $Wy$ and you get the OLS), and thus statistical tests like the LM (and others using the likelihood and other fit statistics) cannot make direct comparisons.  However, Anselin (1996) formulated a set of decision steps using the LM tests to decide between SLM and SEM (and OLS).  The steps are as follows

<br>

1. Run the non robust LM tests. If neither $LM_{lag}$ nor $LM_{err}$ reject the null, go with the OLS.
2. If one of $LM_{lag}$ or $LM_{err}$ rejects the null but the other does not, select the model that rejects the null
3. When both reject the null, turn to the robust LM tests
4. If one of robust $LM_{lag}$ or robust $LM_{err}$ rejects the null but the other does not, select the model that rejects the null
5. When both reject the null, choose the one that is "More significant" (higher LM test statistic)

<br>

Although it is important to run these tests, whether SLM or SEM is most appropriate is really a prior theoretical question, which must be considered relative to the goals of a specific research question. If we expect to see, or are interested in estimating, spatial feedback, then SLM would be a more appropriate model.  For example, if you are interested in understanding whether crime diffuses across neighborhood borders, an SLM is the most appropriate model.  If you are interested in understanding whether airbnb rates in a neighborhood are influenced by the airbnb rates in nearby neighbors, run SLM.  If you are interested in simply controlling for spatial dependency as a nuisance, run an SEM.


To run the LM tests in R, use the `lm.LMtests()` command in the **spatialreg** package.  The argument `test = "all"` runs all the tests.  You should look at *LMerr* (Spatial Error), *LMlag* (Spatial Lag), *RLMerr* (Robust Spatial Error) and *RLMlag* (Robust Spatial Lag).


```{r}
lm.LMtests(fit.ols, listw = nycw, test = "all", zero.policy=TRUE)
```


<p class="comment", style="font-style:normal">**Question 5**: Using the steps outlined by Anselin (1996) above, specify which model is "best"? Why? </p>


***


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)