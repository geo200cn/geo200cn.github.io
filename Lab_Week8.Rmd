---
title: "Lab Week 8"
author: "Kenji Tomari"
output: 
  html_document:
    theme: readable
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Classification

## Question 1

To create the confusion matrix, just use the code above the question, with the proper replacements, eg:

```{r eval=F}
pfit4 <- predict(ols.fit2, type = "response")
brfss16 <- mutate(brfss16, 
                  pprob_ols = pfit4, 
                  pclass_ols = ifelse(pprob_ols > 0.5, 1, 0))
table(brfss16$pclass_ols, 
      brfss16$badhealth)
```

```{r eval=F}
# Calculating the Percent Correctly Predicted for OLS Model

# NOTE
# I just take correctly predicted divided by total observations. 
# Then I multiply it by 100 to get a percentage.

# Predicted False and is Actually False
(length(which(brfss16$pclass_ols == 0 & 
                brfss16$badhealth == 0)) +
   
   # Predicted True and is Actually True
   length(which(brfss16$pclass_ols == 1 & 
                  brfss16$badhealth == 1))) /
  
  # Total Observations
  nrow(brfss16) *
  
  100 # percent
```

Confusion matrices are discussed on page 147 of ISLR. (Look for the label "confusion matrix" on the right hand margin in grey font.)

Note that in answering this question you want to differentiate what "better prediction" means given various error rates. Overall error rate? Type I Error rate? Type II Error rate?

What about the overall error rate of OLS?

```{r eval=F}
# Error Rate

# Predicted True when its Actually False
(length(which(brfss16$pclass_ols == 1 & 
                brfss16$badhealth == 0)) +
   
   # Predicted FALSE when its Actually True
   length(which(brfss16$pclass_ols == 0 & 
                  brfss16$badhealth == 1))) /
  
  # Total Observations
  nrow(brfss16) *
  
  100 # percent
```


## Question 2

```{r eval=F}
# DATA
brfss17 <- read_csv("classification/brfss17.csv")

# Using 2016 Model on 2017 Data
pfit2017 <- predict(logit4.fit, newdata = brfss17, type = "response")

brfss17 <- mutate(brfss17, 
                  pprob = pfit2017, 
                  pclass = ifelse(pprob > 0.5, 1, 0))
```

Now just use the above code from Question 1 to get the error rate (replacing `brfss16` with `brfss17`; and `pclass_ols` with `pclass`).

## Question 3

Use `table` and error rate code from Question 1, but replacing the appropriate arguments (i.e. `test$pclasslda` and `test$badhealth`)

## Question 4

```{r eval=F}
lda.pred_lc <- predict(lda.fit, newdata = test.ca)
test.ca <- mutate(test.ca, pclasslda = lda.pred_lc$class)
```

```{r eval=F}
# Which ones were correclty predicted?
test.ca <- test.ca %>%
  mutate(err_lda = case_when(
    pclasslda == whr ~ "correctly_predicted",
    T ~ "incorrectly_predicted"
  ))

# Test Error Rate with future data.
nrow(test.ca[test.ca$err_lda == "incorrectly_predicted",]) /
  
  # Total Observations
  nrow(test.ca) *
  
  100 # percent
```

## Question 5

```{r eval=F}
# Table of Error Rates by Class
test.ca %>%
  mutate(err_lda = case_when(
    pclasslda == whr ~ 0,
    T ~ 1
  )) %>%
  group_by(whr) %>%
  summarize(total = n(),
            errors = sum(err_lda),
            error_rate = errors/total)
```

## Question 6

```{r eval=F}
# add knn predictions of classes to test.ca
test.ca <- mutate(test.ca, pclassknn = knn.pred)
```

```{r eval=F}
# which are correctly predicted?
test.ca <- test.ca %>%
  mutate(err_knn = case_when(
    pclassknn == whr ~ "correctly_predicted",
    T ~ "incorrectly_predicted"
  ))

# Test Error Rate
nrow(test.ca[test.ca$err_knn == "incorrectly_predicted",]) /
  
  # Total Observations
  nrow(test.ca) *
  
  100 # percent
```

```{r eval=F}
# Table of Error Rates by Class
test.ca %>%
  mutate(err_knn = case_when(
    pclassknn == whr ~ 0,
    T ~ 1
  )) %>%
  group_by(whr) %>%
  summarize(total = n(),
            errors = sum(err_knn),
            error_rate = errors/total)
```

# Resampling

## Question 1

Note that the by running `st_crs()` on the `ca.temp.sf` dataset, we can find out that the map is in units of meters. So, in order to calculate the change in temperature, we need to multiply our coefficient by whatever 500 miles is in meters: approximately 804,672 meters.

Hint, what does 1 meter change in the Y coordinate yield in terms of temperature? You just multiply that value by 804,672 meters.

## Question 2

> "The cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results... The first is the standard k-fold CV estimate, as in (5.3). The second is a bias- corrected version." (ISLR p192-194)

Remember that LOOCV is a special case of k-fold Cross Validation (ISLR p181). (Hence the above quotation says k-fold CV estimate.) What is the delta vector/standard k-fold CV estimate then? In the case of LOOCV, its equation 5.1 on p. 179. You'll note its the average Mean Squared Error (MSE). To get the Root Mean Squared Error (RMSE), one simply takes the squareroot the MSE. In this case, use the first delta value, the standard LOOCV estimate.

```{r eval=F}
# get the data you want to use to model temperature by coordinates.
df2 <- data.frame(st_coordinates(ca.temp.sf), temp = ca.temp.sf$temp)
# simple (OLS) linear model of temperature by coordinates
m2 <- glm(temp ~ X+Y, data=df2)
# Use cv.glms() to calculate the LOOCV estimate.
# Note, we don't set the argument k:
# "The default is to set K equal to the number of observations in data
# which gives the usual leave-one-out cross-validation."
cv_m2 <- boot::cv.glm(data = df2, glmfit = m2)
# Standard k-fold CV estimate, or in this case, LOOCV estimate.
# To get the bias-corrected version, use cv_m2$delta[2]
cv_m2$delta[1]  # FYI, this is not the answer, there's one more step.
```

## Question 3

ISLR discusses how to pick the best model in the section "6.1.3 Choosing the Optimal Model," and discusses adjusted R^2 specifically on p212-3. Even if some of the math is above your head, the "intuition" paragraph should be helpful in answering this question. Also, here is some R code that might help you out.

```{r eval=F}
# Which number of variables has the lowest Adjusted R^2?
which.min(regfit2.fwd.summary$adjr2)
# Which number of variables has the highest Adjusted R^2?
which.max(regfit2.fwd.summary$adjr2)
# Which variables? Just plug the NUMBER you want into:
# coef(regfit2.fwd, NUMBER)
```

## Question 4

Use the below code to use Backward Stepwise Selection to get the best 33 variable models. Then use the same code you used for Question 3 to determine the best model based on adjusted R^2.

```{r eval=F}
regfit2.backward <- regsubsets(physhlth ~ bmi + 
                                 race_eth + 
                                 agec + 
                                 male + 
                                 smoke + 
                                 educ + 
                                 inc + 
                                 employ + 
                                 marst + 
                                 ins + 
                                 bphigh + 
                                 toldhi + 
                                 cvdinfr + 
                                 cvdstrk + 
                                 asthma + 
                                 havarth + 
                                 diabete, 
                               data = brfss17, 
                               method = "backward", 
                               nvmax = 33)
```

## Question 5

If you look at p. 250 in ISLR, the authors demonstrate model selection using k-fold CV with their dataset. Where we found a 28-variable model, they found an 11-variable model was the best. Read the last paragraph on p. 250 carefully, and follow their example. You should, of course, replace the model specification (and other arguments for `regsubsets`) with the one we specify. Then run `coef` with 28 instead of 11 (as they do on the bottom of p. 250).

## Question 6

Do the above code starting with `set.seed(1234)` and replace the value for argument `alpha` with 1 for both `cv.glmnet` and `glmnet`.