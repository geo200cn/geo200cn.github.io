---
title: "Spatial Autocorrelation"
subtitle: <h4 style="font-style:normal">GEO 200CN - Quantitative Geography</h4>
author: <h4 style="font-style:normal">Professor Noli Brazil</h4>
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: yeti
    code_folding: show
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\





[Tobler's First Law of Geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography) states that "Everything is related to everything else, but near things are more related than distant things."  The law is capturing the concept of spatial autocorrelation.  We will be covering the R functions and tools that measure spatial autocorrelation, closely following OSU Chapters 7 and 8. The objectives of the guide are as follows

1. Learn how to create a spatial weights matrix
2. Calculate global spatial autocorrelation
3. Detect clusters using local spatial autocorrelation

To help us accomplish these learning objectives, we will examine housing eviction rates, an issue that has received significant [public attention](https://www.citylab.com/equity/2017/10/where-evictions-hurt-the-most/544238/). 


<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

We'll be introducing one new package in this lab: **spdep**.  


```{r eval = FALSE}
if (!require("spdep")) install.packages("spdep")
```

The other packages we need should be pretty familiar to you at this point.


```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(sp)
library(spdep)
library(tmap)
```


<div style="margin-bottom:25px;">
</div>
## **Why measure spatial autocorrelation**
\

Last lab we dealt with the spatial clustering of points. This lab we're dealing with the spatial clustering of polygons. Or more precisely, the spatial clustering of attributes across polygons. Think about the types of real world geographic objects we can represent with polygons. Neighborhoods are one. People live in neighborhoods. Policy deploys important economic resources at the [neighborhood level](https://www.irs.gov/newsroom/opportunity-zones-frequently-asked-questions). We might then be interested in whether certain phenomena, such as poverty or housing displacement are clustered across neighborhoods in a city. Why? Because concentrations of poverty are harder to break up than pockets it of it. Measuring segregation is another good application of clustering.  People talk about segregation [all](https://www.citylab.com/equity/2019/01/african-american-atlantic-city-segregation-northside-tourism/580576/) [the](https://www.latimes.com/opinion/op-ed/la-oe-rothstein-segregated-housing-20170820-story.html) [time](https://www.nytimes.com/2017/05/15/opinion/school-segregation-nyc.html). But, it is often talked  about in very colloquial or imprecise ways. Well, we have actual (geographic) methods to quantify the degree of segregation in a city.  If you study soils, perhaps your polygons are plots of land.  Are healthy plots clustered? You don't even need to have administrative boundaries or designated plots of lands to measure spatial autocorrelation. For example, [Diniz-Filho, Bini and Hawkins (2003)](https://onlinelibrary.wiley.com/doi/10.1046/j.1466-822X.2003.00322.x) use equal sized cells to measure the clustering of species diversity in Western Paleartic birds.

Similar to the point pattern analysis guide, we won't go into the methods that help us *explain* why something is clustered.  We're just trying to descriptively understand whether our variable of interest is clustered, what is the size or magnitude of the clustering, and where that clustering occurs

Our research questions in this lab are: Do neighborhood housing eviction rates cluster in the Sacramento Metropolitan Area? Where do housing eviction rates cluster?


<div style="margin-bottom:25px;">
</div>
## **Bringing in the data**
\

Let's bring in our main dataset for the lab, a shapefile named *sacmetrotracts.shp*, which contains 2016 court-ordered [housing eviction rates](http://evictionlab.org/) for census tracts in the Sacramento Metropolitan Area. If you would like to learn more about how these data were put together, check out the Eviction Lab's [Methodology Report](https://evictionlab.org/docs/Eviction%20Lab%20Methodology%20Report.pdf). I zipped up the file and uploaded it onto Github.  Set your working directory to an appropriate folder and use the following code to download and unzip the file.  I also uploaded the file in Canvas in the Lab and Assignments Week 5 folder.


```{r eval = FALSE}
#insert the pathway to the folder you want your data stored into
setwd("insert your pathway here")
#downloads file into your working directory 
download.file(url = "https://raw.githubusercontent.com/geo200cn/data/master/sacmetrotracts.zip", destfile = "sacmetrotracts.zip")
#unzips the zipped file
unzip(zipfile = "sacmetrotracts.zip")
```

```{r echo = FALSE}
download.file(url = "https://raw.githubusercontent.com/geo200cn/data/master/sacmetrotracts.zip", destfile = "sacmetrotracts.zip")
unzip(zipfile = "sacmetrotracts.zip")
```


You should see the *sacmetrotracts* files in your current working directory (check `getwd()` to see where your data were downloaded). Bring in the file using `st_read()`. 


```{r message = FALSE, warning = FALSE, results = "hide"}
sac.tracts.sf <- st_read("sacmetrotracts.shp")
```

We'll need to reproject the file into a [Coordinate Referene System](https://geo200cn.github.io/Lab_Week3.html#chapter_6:_coordinate_reference_systems) that uses meters as the units of distance. Let's use UTM Zone 10.


```{r message = FALSE, warning = FALSE}
sac.tracts.sf <-st_transform(sac.tracts.sf, 
                             crs = "+proj=utm +zone=10 +datum=NAD83 +ellps=GRS80") 
```

Last thing we need to do to get the data set ready for analysis is to convert it to an **sp** object.  Use the `as()` function to convert *sac.tracts.sf* from an **sf** to an **sp** object


```{r message = FALSE, warning = FALSE}
sac.tracts.sp <- as(sac.tracts.sf, "Spatial")
```


<div style="margin-bottom:25px;">
</div>
## **Exploratory mapping**
\

Before we calculate spatial autocorrelation, we should first map our variable or interest to see if it *looks* like it clusters across space.  Using the function `tm_shape()`, here is a map of eviction rates in the Sacramento metro area.  


```{r message = FALSE, warning = FALSE}
library(tmap)
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "evrate", style = "quantile",palette = "Reds", 
              title = "") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1, position = c("left", "bottom")) +
  tm_layout(main.title = "Eviction Rates in Sacramento Metropolitan Area Tracts",  
            main.title.size = 0.95, frame = FALSE)
```
  
It does look like eviction rates cluster.  In particular, there appears to be a concentration of high eviction rate neighborhoods in downtown Sacramento, south Sacramento and the northeast portions of the metro area. 

<div style="margin-bottom:25px;">
</div>
## **Spatial weights matrix**
\

Before we can formally model the dependency shown in the above map, we must first establish how neighborhoods are spatially connected to one another.  That is, what does "near" mean when we say "near things are more related than distant things"?   You need to define

1. Neighbor connectivity (who is you neighbor?)
2. Neighbor weights (how much does your neighbor matter?)

A discussion of the spatial weights matrix is provided on page 201 in OSU.

<div style="margin-bottom:25px;">
</div>
### **Neighbor connectivity: Contiguity**
\

Sharing a border and/or vertex is a common way of defining a neighbor.  The two most common ways of defining contiguity is Rook and Queen adjacency (Figure 1).  Rook adjacency refers to neighbors that share a line segment.  Queen adjacency refers to neighbors that share a line segment (or border) or a point (or vertex).


<center>
![Figure 1: Geographic contiguity](/Users/noli/Documents/UCD/teaching/CRD 230/Lab/crd230.github.io/fig1.png)

</center>

<br>

Neighbor relationships in R are represented by neighbor *nb* objects.  An *nb* object identifies the neighbors for each feature in the dataset.  We use the command `poly2nb()` from the **spdep** package to create a contiguity-based neighbor object.   Let's specify Queen adjacency.  The function `poly2nb()` only takes in **sp** objects, so we'll need to use *sac.tracts.sp* here.


```{r message = FALSE, warning = FALSE}
sacb<-poly2nb(sac.tracts.sp, queen=T)
```

You plug the object *sac.tracts.sp* into the first argument of `poly2nb()` and then specify Queen contiguity using the argument `queen=T`. To get Rook adjacency, change the argument to `queen=F`. 

The function `summary()` tells us something about the neighbors. 


```{r message = FALSE, warning = FALSE}
summary(sacb)
```

The average number of neighbors (adjacent polygons) is 6.3, 1 polygon has 1 neighbor and 1 has 18 neighbors.



<div style="margin-bottom:25px;">
</div>
### **Neighbor connectivity: Distance**
\

In distance based connectivity, features within a given radius are considered to be neighbors. The length of the radius is left up to the researcher to decide. For example, [Weisburd, Groff and Yang (2012)](https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195369083.001.0001/acprof-9780195369083) use a quarter mile (approximately 3-4 city blocks) in their study of crime clusters in Seattle. Often studies test different distances to test the robustness of the findings (e.g. [Poulsen et al. 2010](https://journals.sagepub.com/doi/abs/10.1068/a42181)). When dealing with polygons, x and y are the coordinates of their centroids (the center of the polygon). You create a radius of distance *d2* around the observation of interest - other polygons whose centroids fall inside this radius are tagged as neighbors. 

<center>
![Figure 2: Distance based](/Users/noli/Documents/UCD/teaching/CRD 230/Lab/crd230.github.io/fig3.png)

</center>


Other distance metrics besides Euclidean are possible depending on the context and area of your subject. For example, Manhattan distance, which uses the road network rather than the straight line measure of Euclidean distance, is often used in city planning and transportation research.

You create a distance based neighbor object using using the function `dnearneigh()`. The `dnearneigh()` function tells R to designate as neighbors the units falling within the distance specified between `d1` (lower distance bound) and `d2` (upper distance bound). Note that `d1` and `d2` can be any distance value as long as they reflect the distance units that our shapefile is projected in (meters for UTM). The option `x` gives the geographic coordinates of each feature in your shapefile which allows R to calculate distances between each feature to every other feature in the dataset. You get the coordinates using the function `coordinates()`.  The function does not take in **sf** objects, so you'll have to hold hands again with **sp**.


```{r message = FALSE, warning = FALSE}
#extract tract coordinates
sac.coords <- coordinates(sac.tracts.sp)
```

Using `dnearneigh()`, we create a distance based nearest neighbor object where `d2` is 20 miles (32186.9 meters). `d1` will equal 0. `{r}ow.names =` specifies the unique ID for each polygon.

  

```{r message = FALSE, warning = FALSE}
Sacnb_dist1 <- dnearneigh(sac.coords, d1 = 0, d2 = 32186.9, 
                          row.names = sac.tracts.sp$GEOID)
```


<div style="margin-bottom:25px;">
</div>
### **Neighbor connectivity: k-nearest neighbors**
\

Another common method for defining neighbors is k-nearest neighbors. This will find the k closest observations for each observation of interest, where k is some integer. For instance, if we define k=3, then each observation will have 3 neighbors, which are the 3 closest observations to it, regardless of the distance between them. Using the k-nearest neighbor rule, two observations could potentially be very far apart and still be considered neighbors.

<center>
![Figure 2: k-nearest neighbors: k = 3](/Users/noli/Documents/UCD/teaching/CRD 230/Lab/crd230.github.io/fig2.png)

</center>


You create a k-nearest neighbor object using the commands `knearneigh()`and `knn2nb()`. First, create a k nearest neighbor object using `knearneigh()` by plugging in the tract coordinates and specifying *k*.  Then create an *nb* object by plugging in the object created by `knearneigh()` into `knn2nb()`

<div style="margin-bottom:25px;">
</div>
### **Neighbor weights**
\

We've established who our neighbors are by creating an *nb* object.  The next step is to assign weights to each neighbor relationship. The weight determines *how much* each neighbor counts.  You will need to use the `nb2listw()` command to create the weights matrix. Let's create weights for our Queen contiguity defined neighbor object *sacb*  


```{r message = FALSE, warning = FALSE}
sacw<-nb2listw(sacb, style="W", zero.policy= TRUE)
```

In the command, you first put in your neighbor *nb* object (*sacb*) and then define the weights `style = "W"`. Here, `style = "W"` indicates that the weights for each spatial unit are standardized to sum to 1 (this is known as row standardization).  For example, census tract 1 has 3 neighbors, and each of those neighbors will have weights of 1/3. 



```{r message = FALSE, warning = FALSE}
sacw$weights[[1]]
```


This allows for comparability between areas with different numbers of neighbors.

The argument `zero.policy= TRUE` deals with situations when an area has no neighbors based on your definition of neighbor (one example is Catalina Island in Southern California).  When this happens and you don't include `zero.policy= TRUE`, you'll get the following error 

<br>

````
Error in nb2listw(countyb, style = "W") : Empty neighbour sets found
````

<br>


We can visualize the neighbor connections between tracts by plotting the weights matrix


```{r message = FALSE, warning = FALSE}
plot(sac.tracts.sp, border = "grey60")
plot(sacw, coords = sac.coords, add=T, col=2)
```


Let's create the spatial weights matrix for the 20 mile distance


```{r message = FALSE, warning = FALSE}
Sacw_dist1<-nb2listw(Sacnb_dist1, style="W", zero.policy= TRUE)
```

The plot of connections for the 20 mile distance based neighbor object and weights matrix yields


```{r message = FALSE, warning = FALSE}
plot(sac.tracts.sp, border = "grey60")
plot(Sacw_dist1, coords = sac.coords, add = T, col = 2)
```



<div style="margin-bottom:25px;">
</div>
## **Moran Scatterplot**
\

We've now defined what we mean by neighbor by creating an *nb* object and the influence of each neighbor by creating a spatial weights matrix.  The first map we created in this guide showed that neighborhood eviction rates appear to be clustered in Sacramento. We can visually explore this a little more by plotting standardized eviction rates on the x-axis and the standardized average eviction rate of one's neighbors (also known as the spatial lag) on the y-axis.  This plot is known as a Moran scatterplot.  Let's create one using the Queen based spatial weights matrix.   Fortunately, **spdep** has the function `moran.plot()` to help us out.


```{r message = FALSE, warning = FALSE}
moran.plot(sac.tracts.sp$evrate, listw=sacw, xlab="Standardized Eviction Rate", ylab="Standardized Lagged Eviction Rate",
main=c("Moran Scatterplot for Eviction Rate", "in Sacramento I=.0.425") )
```


Looks like a fairly strong positive association - the higher your neighbors' average eviction rates, the higher your eviction rate.  


<div style="margin-bottom:25px;">
</div>
## **Global spatial autocorrelation**
\

The map and Moran scatterplot provide descriptive visualizations of clustering (autocorrelation) in eviction rates.  But, rather than eyeballing the correlation, we need a quantitative and objective approach to quantifying the degree to which similar features cluster.  This is where global measures of spatial autocorrelation step in.  A global index of spatial autocorrelation provides a summary over the entire study area of the level of spatial similarity observed among neighboring observations.  

<div style="margin-bottom:25px;">
</div>
### **Moran's I**
\

The most popular test of spatial autocorrelation is the Global Moranâ€™s I test, which is discussed on page 205 in OSU.  Use the command `moran.test()` in the **spdep** package to calculate the Moran's I.  You specify the variables from the **sp** object and the spatial weights matrix. Let's calculate Moran's I using our Queen contiguity spatial weights matrix.


```{r message = FALSE, warning = FALSE}
moran.test(sac.tracts.sp$evrate, sacw)    
```


We find that the Moran's I is positive (0.57) and statistically significant (p-value < 0.01). Remember from lecture that the Moran's I is simply a correlation, and correlations go from -1 to 1.  A 0.54 correlation is fairly high - OSU uses a rule of thumb of correlations higher than 0.3 and lower than -0.3 as meaningful.  Moreover, we find that this correlation is statistically significant (p-value basically at 0).

We can compute a p-value from a Monte Carlo simulation as is discussed on page 208 in OSU using the `moran.mc()` function.  


```{r message = FALSE, warning = FALSE}
moran.mc(sac.tracts.sp$evrate, sacw, nsim=999, zero.policy= TRUE)
```



The only difference between `moran.test()` and `moran.mc()` is that we need to set `nsim=` in the latter, which specifies the number of random simulations to run.  We end up with a p-value of 0.001.  You can use the distance and k-nearest neighbor based definitions to test how sensitive the results are to different definitions of neighbor. 


<div style="margin-bottom:25px;">
</div>
### **Geary's c**
\

Another popular index of global spatial autocorrelation is Geary's c which is a cousin to the Moran's I. Similar to Moran's I, it is best to test the statistical significance of Geary's c using a Monte Carlo simulation. You use `geary.mc()` to calculate Geary's c.  OSU discusses Geary's c on page 211.


<div style="margin-bottom:25px;">
</div>
## **Local spatial autocorrelation**
\

The Moran's I tells us whether clustering exists in the area.  It does not tell us, however, *where* clusters are located.  These issues led spatial scholars to consider local forms of the global indices, known as Local Indicators of Spatial Association (LISAs).

LISAs have the primary goal of providing a local measure of similarity between each unit's value (in our case, eviction rates) and those of nearby cases.  That is, rather than one single summary measure of spatial association (Moran's I), we have a measure for every single unit in the study area.  We can then map each tract's LISA value to provide insight into the location of neighborhoods with comparatively high or low associations with neighboring values (i.e. hot or cold spots).

<div style="margin-bottom:25px;">
</div>
### **Getis-Ord**
\

A popular local measure of spatial autocorrelation is Getis-Ord, which is discussed on page 219 in OSU.  There are two versions of the Getis-Ord, $G_i$ and $G_i^*$.  $G_i$ only uses neighbors to calculate hot and cold spots whereas $G_i^*$ includes the location itself in the calculation. Let's calculate $G_i^*$.  To do this, we need to use the `include.self()` function. We use this function on *sacb* to create an *nb* object that includes the location itself as one of the neighbors.  


```{r message = FALSE, warning = FALSE}
sacb.self <- include.self(sacb)
```

We then plug this new self-included *nb* object into `nb2listw()` to create a self-included spatial weights object


```{r message = FALSE, warning = FALSE}
sac.w.self <- nb2listw(sacb.self, style="W", zero.policy= TRUE)
```

We calculate $G_i^*$ for each tract using the function `localG()` which is part of the **spdep** package. You plug in the variable you want to find clusters for and the self-included spatial weights object `sac.w.self()`


```{r message = FALSE, warning = FALSE}
localgstar<-localG(sac.tracts.sp$evrate,sac.w.self, zero.policy = TRUE)
```


The command returns a *localG* object containing the Z-scores for the $G_i^*$ statistic. 


```{r message = FALSE, warning = FALSE}
summary(localgstar)
```


Local Getis-Ord has returned z-scores between -3.80897 and 9.47085.  This statistic can describe where hot and cold spots cluster. The interpretation of the Z-score is straightforward: a large positive value suggests a cluster of high eviction rates (*hot spot*) and a large negative value indicates a cluster of low eviction rates (*cold spot*). 

In order to plot the results, you'll need to coerce the object *localgstar* to be numeric.  Let's do that and save this numeric vector into our **sf** object *sac.tracts.sf*. 


```{r message = FALSE, warning = FALSE}
sac.tracts.sf <- mutate(sac.tracts.sf, localgstar = as.numeric(localgstar))
```

We can create a map of the $G_i^*$ values like OSU does in Figure 8.1 on page 221 (their map is for the regular $G_i$).  We'll break up the values by quintile.


```{r message = FALSE, warning = FALSE}
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "localgstar", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
```


You show more spread of $G_i^*$  by showing deciles


```{r message = FALSE, warning = FALSE}
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "localgstar", title = "Gi* value", palette = "-RdBu", style = "quantile", n=10) +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
```


The argument `palette = "-RdBu"` reverses the colors such that higher values are red (hot) and lower values are blue (cold). 

The values you get from `localG()` are Z scores. We might interpret Z scores outside -1.96 and 1.96 as unusual cases, or statistically significant hot spots at the 95% level (remember that -1.96 and 1.96 represent the values on the standard normal distribution that correspond to a 95% confidence interval).  However, OSU on page 221 warns that normality in the distribution of local statistics is often not met.  Nevertheless, let's see where hot and cold spots are located using Z values that are associated with 99, 95 and 90 percent confidence intervals. 

We can create a vector named *breaks* to designate the cutoff points at the different significance levels (1% (or 99%), 5% (or 95%), and 10% (or 90%)) using the appropriate Z-scores. Set `-Inf` and `Inf` as the floor and ceiling, respectively.


```{r message = FALSE, warning = FALSE}
breaks <- c(-Inf, -2.58, -1.96, -1.65, 1.65, 1.96, 2.58, Inf)
```

Then map the clusters using `tm_shape()` using *breaks* for the `breaks =` argument.


```{r message = FALSE, warning = FALSE}
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "localgstar", title = "Gi* value", palette = "-RdBu",
              breaks = breaks) +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
```


We can create a categorical variable within *sac.tracts.sf* that designates tracts as cold, hot and not significant by using the `cut()` function inside `mutate()`.  


```{r message = FALSE, warning = FALSE}
sac.tracts.sf<-  mutate(sac.tracts.sf, gcluster = cut(localgstar, breaks=breaks, include.lowest = TRUE, labels=c("Cold spot: 99% confidence", "Cold spot: 95% confidence", "Cold spot: 90% confidence", "Not significant","Hot spot: 90% confidence", "Hot spot: 95% confidence", "Hot spot: 99% confidence"))) 
```

We can then map that variable. We get the same map as above, but now with labels.


```{r message = FALSE, warning = FALSE}
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "gcluster", title = "", palette = "-RdBu",
              breaks = breaks) +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
```


We can also eliminate the different significance levels and simply designate hot and cold spots as tracts with Z-scores above 1.96 and below -1.96 (95% significance level).


```{r message = FALSE, warning = FALSE}
breaks <- c(-Inf, -1.96, 1.96, Inf)
sac.tracts.sf<-  mutate(sac.tracts.sf, gcluster = cut(localgstar, breaks=breaks, include.lowest = TRUE, labels=c("Cold spot", "None", "Hot spot"))) 
```

And then map


```{r message = FALSE, warning = FALSE}
sac.ev.map.g <- tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "gcluster", title = "", palette = "-RdBu",
              breaks = breaks) +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
sac.ev.map.g
```


Let's put it into an interactive map.  Where do high eviction rate neighborhoods cluster? Zoom in and find out.


```{r message = FALSE, warning = FALSE}
tmap_mode("view")
sac.ev.map.g + tm_view(basemaps="OpenStreetMap")
```



<div style="margin-bottom:25px;">
</div>
### **Local Moran's I**
\

Another popular measure of local spatial autocorrelation is the local Moran's I, which is discussed on page 222 in OSU.  We can calculate the local Moran's I using the command `localmoran()` found in the **spdep** package. 


```{r message = FALSE, warning = FALSE}
locali<-localmoran(sac.tracts.sp$evrate, sacw, p.adjust.method="bonferroni")
```

The argument `p.adjust.method="bonferroni"` adjusts for the multiple testing problem that OSU describes on page 224. The resulting object is a matrix with 5 columns - the local statistic, the expectation of the local statistic, the variance, the Z score (deviation of the local statistic from the expectation divided by the standard deviation), and the p-value. Save the local statistic and the Z-score into our sf object *sac.tracts.sf* for mapping purposes


```{r message = FALSE, warning = FALSE}
sac.tracts.sf <- mutate(sac.tracts.sf, localmi = locali[,1], localz = locali[,4])
```

We have to make our own identifiers for statistically significant clusters. Let's designate any areas with Z-scores greater than 1.96 or less than -1.96 as high and low clusters, respectively. 


```{r message = FALSE, warning = FALSE}
sac.tracts.sf <- mutate(sac.tracts.sf, mcluster = cut(localz, breaks = c(min(localz),-1.96, 1.96, max(localz)), include.lowest = TRUE, labels = c("Negative Correlation", "Not Significant", "Positive Correlation")))
```

Now we map!


```{r message = FALSE, warning = FALSE}
tm_shape(sac.tracts.sf, unit = "mi") +
  tm_polygons(col = "mcluster", title = "", palette = "-RdBu",
              breaks = breaks) +
  tm_scale_bar(breaks = c(0, 10, 20), size = 1) +
  tm_compass(type = "4star", position = c("left", "bottom")) + 
  tm_layout(frame = F, main.title = "Sacramento eviction clusters",
            legend.outside = T) 
```


Positive values indicate similarity between neighbors while negative values indicate
dissimilarity. This means that high values of $I_i$ indicate that similar values are being clustered. In contrast, low values of $I_i$ indicate that dissimilar (high and low) values are clustered. When working with the local Moran's *I*, you will want to link it back to the four quadrants of the Moran scatterplot where we identified High-High, Low-Low, High-Low, and Low-High clusters (described on page 222 in OSU). This will allow you to create the type of map produced on page 11 in [Scott et al. (2010)](https://www.tandfonline.com/doi/full/10.1080/00330120903375837).


***


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)

