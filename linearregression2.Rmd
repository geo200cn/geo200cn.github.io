---
title: "More Linear Regression"
subtitle: <font size="4">GEO 200CN - Quantitative Geography</font>
author: Professor Noli Brazil
date: April 22, 2024
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    theme: cosmo
    code_folding: show
    self_contained: false
---


<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: normal;
}

.figure {
   margin-top: 20px;
   margin-bottom: 20px;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 13px;
  font-family: Arial;
}
</style>


\

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```


We're in the second leg of our journey into linear regression. In [last lab](https://geo200cn.github.io/linearregression.html), we learned about simple linear regression and model fit.  In this lab, we go through the R functions for running regression diagnostics and multiple linear regression. The objectives of this lab are as follows

1. Learn how to run diagnostic tools to evaluate Ordinary Least Squares (OLS) regression assumptions
2. Learn how to run and evaluate a multiple linear regression model
3. Learn how to detect multicollinearity


To help us accomplish these learning objectives, we will continue examining the association between neighborhood characteristics and housing eviction rates in the Sacramento metropolitan area. We'll be closely following the material presented in Handout 4.


<div style="margin-bottom:25px;">
</div>
## **Installing and loading packages**
\

We'll be using a couple of new packages in this lab.  First, you'll need to install them if you have not already done so.

```{r eval = FALSE}
install.packages(c("GGally", "lmtest", "car"))
```

Load these packages and others we will need for this lab.

```{r warning=FALSE, message=FALSE}
library(MASS)
library(tidyverse)
library(sf)
library(gridExtra)
library(GGally)
library(car)
library(lmtest)
```



<div style="margin-bottom:25px;">
</div>
## **Bringing in the data**
\

Download the csv file *sac_metro_eviction.csv* located on Canvas in the Week 4 Lab and Assignments folder.  Read in the csv file using the `read_csv()` function.

```{r warning=FALSE, message=FALSE}
sac_metro <- read_csv("sac_metro_eviction.csv")
```

2017 eviction rate case data were downloaded from the [Eviction Lab website](https://evictionlab.org/).  Socioeconomic and demographic data were downloaded from the 2013-2017 [American Community Survey](https://www.census.gov/programs-surveys/acs). A record layout of the data can be found [here](https://raw.githubusercontent.com/geo200cn/data/master/sacmetroeviction.txt).  Our research question in this lab is: What ecological characteristics are associated with neighborhood eviction rates in the Sacramento metropolitan area?


<div style="margin-bottom:25px;">
</div>
## **Checking OLS assumptions**
\

The linear regression handout outlines the core assumptions that need to be met in order to obtain unbiased regression estimates from an OLS model.  The handout also goes through several diagnostic tools for examining whether an OLS model breaks these assumptions. In this section, we will go through how to run most of these diagnostics in R. 

Let's also create a fake dataset that meets the OLS assumptions to act as a point of comparison along the way. We'll call this the *goodreg* model.

```{r}
#sets a seed so the following is reproducible
set.seed(08544)
x <-rnorm(5000, mean = 7, sd = 1.56)# just some normally distributed data

## We're establishing here a linear relationship,
## What's this "true" linear relationship we're setting up?
## So that y = 12 - .4x + some normally distributed error values
y <- 12 - 0.4*x +rnorm(5000, mean = 0, sd = 1)

goodreg <- lm(y ~ x)
summary(goodreg)
```

To be clear, we know the exact functional form of this model, all OLS assumptions should be met, and therefore this model should pass all diagnostics.

In the last lab guide, we ran a simple regression model using ordinary least squares (OLS) to estimate the relationship between eviction rates per 100 renting households and percent unemployed at the neighborhood level. Let's run this model using `lm()` and save its results in an object named *lm1*. Our dependent variable is *evict* and the independent variable is *punemp*.

```{r}
#eliminate scientific notation
options(scipen=999)

lm1 <- lm(evict ~ punemp, 
          data = sac_metro)

#results
summary(lm1)
```



<div style="margin-bottom:25px;">
</div>
### **Normally distributed errors**
\

We can rely on several tools for testing the errors are normally distributed assumption.   The first is a histogram of residuals.   We can extract the residuals from an *lm* object using the function `resid()`. We will need to use the residuals for other diagnostics, so let's save them into the *sac_metro*  data frame under the variable *resid* using the `mutate()` function.

```{r}
sac_metro <- sac_metro %>%
            mutate(resid = resid(lm1))
```

The order of the tracts in`resid(lm1)` is the same as the order of the tracts in *sac_metro* and that's why we were able to directly column bind it like we did in the above code.

Now, we create a histogram of residuals using our best bud `ggplot()` which we met in [Week 2](https://geo200cn.github.io/eda.html#Data_visualization).

```{r}
sac_metro %>%
  ggplot() + 
  geom_histogram(mapping = (aes(x=resid))) + 
  xlab("Absolute Residuals")
```


We're trying to see if its shape is that of a normal distribution (bell curve). This is a histogram of absolute residuals. To get a histogram of standardized residuals use the function `stdres()`, where the main argument is our model results *lm1*

```{r warning = FALSE}
sac_metro %>%
  ggplot() + 
  geom_histogram((aes(x=stdres(lm1)))) + 
  xlab("Standardized Residuals")
```

You can also plot a histogram of the studentized residuals using the function `rstudent()`

```{r warning = FALSE}
sac_metro %>% ggplot() + 
  geom_histogram((aes(x=rstudent(lm1)))) + 
  xlab("Studentized Residuals")
```

For comparison, the following is what the residuals from our simulated good data look like 

```{r}
ggplot() + geom_histogram(aes(x = stdres(goodreg))) +
  xlab("Standardized Residuals") +
  ggtitle("Distribution of Residuals - Simulated Data")
```


You can also examine a normal probability plot, also known as a Q-Q plot, to check error normality. Use the function `qqnorm()` and just plug in the model residuals. The function `qqline()` adds the line for what normally distributed data should theoretically follow.


```{r}
qqnorm(sac_metro$resid)
qqline(sac_metro$resid,col="red")
```

In short, if the points of the plot do not closely follow a straight line, this would suggest that the data do not come from a normal distribution.  What does the Q-Q plot look like for our good model?

```{r}
qqnorm(stdres(goodreg))
qqline(stdres(goodreg),col="red")
```

<br>

<p class="comment">**Question 1**: What do you conclude by visually examining the histogram and Q-Q plot of the *lm1* residuals?</p>

<br>

Histograms and Q-Q plots give a nice visual presentation of the residual distribution, however if we are interested in formal hypothesis testing, there are a number of options available. A commonly used test is the Shapiroâ€“Wilk test, which is implemented in R using the function `shapiro.test()`. The null is that the data are normally distributed.  Our good model *goodreg* should not reject the null

```{r}
shapiro.test(resid(goodreg))
```

What about our simple linear regression model?

```{r}
shapiro.test(resid(lm1))
```

What's the conclusion?

<div style="margin-bottom:25px;">
</div>
### **Residual scatterplots**
\

You can use a plot of the residuals against the fitted values for checking both the linearity and homoscedasticity assumptions.  We should look for three things in this plot.

* At any fitted value, the mean of the residuals should be roughly 0. If this is the case, the linearity assumption is valid. For this reason, we generally add a horizontal line in the plot at y = 0 to emphasize this point.
* At every fitted value, the spread of the residuals should be roughly the same. If this is the case, the homoscedasticity (equal variance) assumption is valid.
* There should be no pattern in the residuals.

We know what diagnostic plots should look like when we have good data. But what about for bad data? Below is an example of some bad data that breaks the linearity assumption. Don't worry too much about the intricacies of the code below - we're creating simulated data that is deliberately not normal so you can see what nonlinearity looks like in the context of the diagnostic tools we've been running.

```{r}
#set a random seed so reproducible
set.seed(42)
sim_3 = function(sample_size = 500) {
  x = runif(n = sample_size) * 5
  y = 3 + 5 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 5)
  data.frame(x, y)
}

sim_data_3 = sim_3()
badreg = lm(y ~ x, 
            data = sim_data_3)
```

Here is the residual vs fitted values plot for *badreg*. What do you see that let's you know this model breaks OLS assumptions?

```{r}
plot(fitted(badreg), resid(badreg), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from Bad Model")
abline(h = 0, col = "darkorange", lwd = 2)
```

<br>

<p class="comment">**Question 2**: Create a residual against fitted value plot as described by Handout 4 for the *lm1* model.  Do the same for the *goodreg* model. What do you conclude from these plots?</p>

<br>

<div style="margin-bottom:25px;">
</div>
## **Multiple linear regression**
\

A simple linear regression is, well, too simple.  You'll want to add more variables in your model to

1. Obtain more precise predictions
2. Examine the relationship between the response and more than one variable
3. Control for variables that are confounding the relationship between *X* and *Y*. 

Reason (3) is particularly important for avoiding violations of the OLS assumptions.  Let's go through this reason first to help motivate why to include more than one variable in the model.


<div style="margin-bottom:25px;">
</div>
### **Controlling for variables**
\

The most common reason why your model is breaking OLS assumptions is because you've failed to include a variable that is confounding the relationship between your primary independent variable(s) and the outcome.  Here, we are interested in examining the impact of a variable X on the outcome Y controlling for the impact of another variable Z.  In other words, we don't really care about the effect of Z, but simply want to control for it so we can get an unbiased estimate of the effect of X.  In this case, Z is a confounding variable.  Let's try to make clear what we mean by confounding.  Here are three ways to define a confounding variable, all saying the same thing, but in different ways. 

* Confounding variables or confounders are often defined as variables that correlate (positively or negatively) with both the dependent variable and the independent variable

* A confounder is an extraneous variable whose presence affects the variables being studied so that the results do not reflect the actual relationship between the variables under study.

*  A third variable, not the dependent (outcome) or main independent(exposure) variable of interest, that distorts the observed relationship between the exposure and outcome. 

Confounding can have serious consequences for your results. Going back to our case study of Sacramento let's say we ran a simple linear regression of eviction rates on the median age of housing units in the neighborhood.

```{r}
summary(lm(evict ~  medage, 
           data = sac_metro))
```

We would conclude that a one year increase in the median age of housing units is associated with an increase of 0.01104 eviction cases per 100 renting households. The results also show that the coefficient has a p-value of 0.00975, which indicates that the *medage* coefficient is statistically significant at the 0.01 level. This means that the probability that the association between *medage* and *evict* is due to chance is 100*0.00975 = 0.975 percent.  In other words, the probability of seeing the association 0.01104 just by chance if the null hypothesis is true is a little less than one percent.

But, when you include median gross rent, you get

```{r}
summary(lm(evict ~ medage + rent, 
           data = sac_metro))
```


<br>

<p class="comment">**Question 3**:  Write the equation of the regression line for the model above. What is the interpretation of the slope coefficient for *rent*?  What about the slope coefficient for the intercept?</p>

<p class="comment">**Question 4**: In your own words, explain why the statistically significant relationship between median age of housing units and eviction rates disappeared when we included median rent.</p>

<br>



This example illustrates the importance of accounting for potential confounding in your model.  This includes confounding introduced by spatial dependency or autocorrelation, which we will discuss later in the quarter.  




<div style="margin-bottom:25px;">
</div>
## **Multicollinearity**
\

It might seem that if confounding is such a big problem (and it is when trying to make causal inferences)  you should aim to try to control for *everything.*  Including the kitchen sink.  The downside of this strategy is that including too many variables will likely introduce multicollinearity in your model.  Multicollinearity is defined to be high, but not perfect, correlation between two independent variables in a regression.

What are the effects of multicollinearity? Mainly you will get blown up standard errors for the coefficient on one of your correlated variables.  In other words, you will not detect a relationship even if one does exist because the standard error on the coefficient is artificially inflated.

What to do? First, run a correlation matrix for all your proposed independent variables.  Let's say we wanted to run an OLS model including *pblk*, *phisp*, *pund18*, *totp*, *punemp*, *rent*, *vacancy*, *medage* and *pburden* as independent variables. One way of obtaining a correlation matrix is to use the `cor()` function.  We use the function `select()` to keep the variables we need from *sac_metro*. We use the `round()` function to round up the correlation values to two significant digits after the decimal point.

```{r}
round(cor(dplyr::select(sac_metro, pblk, phisp, pund18, punemp, totp, rent, vacancy, medage, pburden)),2)
```

Any correlation that is high is worth flagging.  In this case, we see a few pairwise correlations that are close to 0.5 that might be worth keeping in mind.

You can also run your regression and then detect multicollinearity in your results.  Signs of multicollinearity include

* Large changes in the estimated regression coefficients when a predictor variable is added or deleted
* Lack of statistical significance despite high $R^2$
* Estimated regression coefficients have an opposite sign from predicted

A formal and likely the most common indicator of multicollinearity is the Variance Inflation Factor (VIF). Use the function `vif()` in the **car** package to get the VIFs for each variable.  Let's check the VIFs for the proposed model.  First, run the model and save it into *lm2*. 

```{r}
lm2 <- lm(evict ~  pblk + phisp + pund18 + punemp + totp + rent+ vacancy + 
            medage + pburden, 
          data = sac_metro)
```

Then get the VIF values using `vif()`. As described in the handout, another measure of multicollinearity - tolerance - can be obtained from the VIF values.

<br>

<p class="comment"> **Question 5**: Assess the presence of multicollinearity using the VIF. </p>

<br>


<div style="margin-bottom:25px;">
</div>
## **Goodness of fit**
\

We can measure the overall fit of a multiple linear regression model by looking at the multiple $R^2$. The multiple $R^2$ is the square of the correlation between the observed values of
Y and the values of Y predicted by the multiple regression model. Therefore, large values of multiple $R^2$ represent a large correlation between the predicted and observed values of the outcome. A multiple $R^2$ of 1 represents a situation in which the model perfectly predicts the observed data. As such, multiple $R^2$ is a gauge of how well the model predicts the observed data. It follows that the resulting $R^2$ can be interpreted in the same way as in simple regression: it is the amount of variation in the outcome variable that is accounted for by the model.

$R^2$ is located in the summary of the regression model. Let's run a regression of eviction rates on percent black and percent unemployment and save the results in an object called *lm3*.

```{r}
lm3 <- lm(evict ~  pblk + punemp, 
          data = sac_metro)
summary(lm3)          
```


Let's compare the model fit to the model we ran earlier that contained more variables 

```{r}
summary(lm2) 
```

<br>

<p class="comment">**Question 6**:  Which model yields a better fit, lm3 or lm2? Explain why. </p>

<p class="comment">**Question 7**:  For the model you picked in Question 6, run diagnostics to determine whether it meets the OLS assumptions of normally distributed errors, homoscedasticity, and linearity. Summarize the results.  </p>




***

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.


Website created and maintained by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)